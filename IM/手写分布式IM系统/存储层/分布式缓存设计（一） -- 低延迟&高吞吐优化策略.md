## 前置阅读
[企业微信的IM架构设计揭秘：消息模型、万人群、已读回执、消息撤回等-IM开发/专项技术区 - 即时通讯开发者社区!](http://www.52im.net/forum.php?mod=viewthread&tid=3631&highlight=%C6%F3%D2%B5%CE%A2%D0%C5%B5%C4IM%BC%DC%B9%B9%C9%E8%BC%C6%BD%D2%C3%D8)

## 背景分析
上述微服务划分过程中，已经确定了三个核心的领域服务，消息，用户，会话。基础设施层，确定了一个关系存储服务。现在需要继续确认IM系统数据结构的设计，这一设计应该充分考虑在**领域服务内存/存储层缓存/存储层持久层中**同一聚合实体的不同存储方式，聚合与实体的设计属于数据结构设计的一个环节。

## 设计目标
1. 存储数据，用于支持基本查询：
	a.消息，状态（消息可见性，已读，撤回，登陆状态）
	b.关系（uid到uid，uid到sessionlD，sessionlD到uid，uid到did）
2. 支持历史消息的查询，新设备登陆后会自动同步7关内的历史消息到新的客户端。

## 评估标准
1. 低延迟优化p99 10ms以下 
2. 高吞吐 每秒下发 10M/s
3. 高可用 5个9，5分钟内修复问题
4. 业务层面的外部一致性 
5. 尽可能的降低存储成本
6. 易于水平扩展，支持多数据中心部署，且便于运维（重启迁移，可观测性，自动化能力，手动干预）

## 方案选择
### 1. 实现基本的设计目标，无缓存方案

| 方案                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 收益                                                                          | 代价                                                                                                                                                            |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ==数据库表的拆分==<br>建设专用的三张关系表，user_to_user, user_to_device, user_to_session<br>建设一张消息表，msg(userlD, sessionlD, msglD, seqlD, content, type， is_delete)<br>建设一张消息状态表，msg_state(userlD, msglD, type) <br>**上行消息到来时**，异步写入消息表一条记录。<br>**下行消息下发时**，通过user_to_session表，以sessionlD反查询到userlD列表，然后再通过多个userlD查询user_to_device表，查询到did list。然后进行消息打包与分发<br>**离线消息同步时**<br>1. 在消息状态表中维护消息是否已经分发的状态，在下行消息ack时更新msg_state表 <br>2. 拉取离线消息时，从msg表拉取消息并查询msg_state表得到未分发过的消息列表 <br>3. 打包后批量下发给客户端<br>**拉取已读列表时**，直接查询msg_state表，根据msglD查询到对其已读的userlD列表，未读列表取反即可。<br>**消息撤回**，异步update msg表的is_delete字段即可，离线消息下发时会去查询状态，保证数据一致性。<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/bc6461cf9139dbc0e0ce2d73539d8b7b_MD5.jpeg) | 1. 实现简单，保证一致性<br>2. 遵循数据库设计范式，没有穴余存储 <br>3. 不存在写放大问题<br>4. 适用小规模app，业务可灵活扩展 | 1. 大规模群聊场景，查询存在极高的读放大问题<br>2. 读时拉取虽然能保证消息状态的及时性但增加了其接口延迟，查库操作非常频繁<br>3. 消息的记录数量是巨大的，而消息的状态是针对用户维度的，用户数x消息数为消息状态的存储上界，这造成极大的存储压力<br>4. 如此高QPS的查询OLTP数据库，是不可行的。 |

### 2. 低延迟优化p99 5ms以下

| 方案                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 收益                                                                                                                                                                                                     | 代价                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ==使用cache==<br>缓存是**解决性能与稳定性的系统设计工具，提供高性能且稳定的数据源，缓存热点函数的中间状态，使得计算路径变短**。<br>业务缓存的设计模式<br>> DB泛指数据源，cache泛指**快速路径上的局部数据源**<br>1. 旁路：写时：更新DB，删除cache；写时miss后查DB回写cache,用于高一致性的场景<br>2. 穿透：写时：hit则更新DB和cache，miss仅更新DB；读时：miss则查询DB并回写，适用于冷热分区 <br>3. 异步：写时：只更新cache，异步更新DB，读时：miss后查询DB并回写，适用于高频写<br>4. 兜底：写时：直接写DB，读时：读DB失败后读cache/成功后回写cache，适用于高可用场景<br>5. 只读：写时：更新DB，读时，读cache，其他异步方式更新cache，适用于100%命中率场景，最终一致场景 <br>6. 回源：写时：写DB，读时：读DB，适用于缓存的降级时期。<br><br>**离线同步消息（穿透模式）**<br>1. 下发消息时，则将消息写入msg表，然后再写入redis中的离线消息队列(**redis的zset，key是sessionlD，value是msgID**）<br>2. 当用户客户端拉取离线消息时，从redis中拉取该sessionID对应的msglD list，然后查询msgID的信息与对于此用户的状态，打包后返回给客户端<br>3. zset默认存储最近100条，超过的部分查询DB，并追加更新缓存最大到1000条。<br><br>**对各种映射关系维护在内存中提高存取性能（只读模式）**<br>1. 维护userlD到sessionlD的正排，在用户创建一个会话时写入到redis的zset中 <br>2. 维护sessionlD到userlD的倒排，在用户创建一个会话时写入到redis的zset中<br>3. 维护userlD到did的正排，在用户使用某个设备登陆时，业务识别为设备登出时删除<br>4. 基于flink等实时计算组件，实时数据流式更新状态，并基于flink的checkpoint机制提供回可恢复性<br><br>**对消息状态进行缓存（旁路模式）**<br>1. 将消息的状态信息存储在redis中，通过msglD可以查询到其对应的value信息(state)<br>2. 在状态发生变更时发送del命令将其缓存删除，进行回源 <br>3. 如果key不存在，则去DB查询并写入到缓存<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/5e069950f98aa4320442d7f8b6885dcb_MD5.jpeg)<br> | 1. 使用内存换取高性能，不落盘存储，相对于DB性能得到极大的提升。<br>2. 对消息状态使用删除缓存的方式，来保证其缓存数据的一致性。 <br>3. 针对正倒排数据使用流处理的方式进行更新，保证了更新的稳定性<br>4. 离线数据采用冷热数据分离的方式，兼顾了命中率与数据规模的矛盾。<br>5. 只读模式的cache设计，保证了百分百的映射关系命中率，避免出现个用户无法收到消息的极端问题。 | 1. **大量的使用缓存，必然存在一致性和可用性问题需要解决** <br>2. sessionlD到userID的映射在聊天室场景，其增长是不收敛的，不可避免的造成**大key**<br>3. 热点群聊场景查询sessionlD到userlD的倒排，会成为极度的**热key**，耗尽redis资源。 <br>4. 对于群聊消息状态的缓存，某条消息频繁更新阅读状态，导致缓存被频紧删除，消息状态查询直接穿透到DB，造成**缓存击穿**。<br>5. 如果某个msgID被撤回，但离线消息拉取的时候多个用户都会查询此msgID的消息状态，此时会造成**缓存穿透**<br>6. 大量的kv存储在redis中，势必造成redis后台任务的频繁触发，导致**慢查询**，核心服务造成抖动。<br>7. 大量的数据存储在redis中，内存的使用将产生**巨天成本**。 <br>8. 消息下发，一次请求需要查询多个key且value巨大，分布式redis场景下会造成**多个key的扇出请求**，成为性能瓶颈。<br>9. 消息状态与离线消息的更新会产生大量的缓存更新，但数据源的异构特性，导致其更新失败时不能保证原子性，**造成不一致**。<br>10. 重度依赖redis，需要考虑redis跨数据中心的一致性问影 |
| ==大key问题==<br>多大算大key？<br>1. String 类型 value长度超过10kb<br>2. 集合类型 元素个数超过5000个<br>3. 集合类型 单元素总长度超过10MB<br>解决方案？通常有：拆分key、压缩key以及使用持久性kv<br><br>针对im的场景，基于业务去优化，最佳的方案就是session绑定机制<br>1. 当聊关室有用户进入时，告知业务server<br>2. 有用户进入即天室则就调接入层，将sessionlD和did的关系绑定到网关机本地内存中<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/efe9e7c6af96019f0d2212b43916f60a_MD5.jpeg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 增加了本地网关机的内存消耗，但却解决了大key问题                                                                                                                                                                              | 增加了整个系统的依赖程度，交互更为复杂                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ==热key问题==<br>降频<br>1. 热key 多存几个副本<br>2. 本地LRU，LFU缓存热key<br>3. 业务上拆分key的作用<br>止损<br>1. 对热key限流<br>2. 能够手动禁用热key<br>3. 热key降级兜底策略<br><br>1. 对于普通群聊，可以对热key缓存到本地，但是本地的缓存应该如何更新（场景要求强一致）？**秒级过期**？<br>2. 消息状态采用旁路缓存，本地缓存解决高热问题，磁盘缓存进一步加大热ke存储容量并解决本地缓存预热问题，远端分布式缓存解决命中率问题。<br>3. 对热key要能 **禁止，限流，降级**。<br>4. 对于只读缓存，无需考虑缓存同步问题，直接读取多个只读副本来缓解热key问题<br>5. 对于旁路缓存，要求一致性，存储多副本需要考虑同步问题，此时可以考虑对消息状态进行拆解，比如：消息已读和消息可见性拆成两个独立的key，以便于下游分布式缓存的分片路由，打散热key。<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/062d56b76790c4b03d2afc6215c843c4_MD5.jpeg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 场景覆盖全面，从设计上避免了热key的存在，解决了延迟抖动的问题，使得缓存层的负载达到均衡                                                                                                                                                          | 1. 增加本地缓存，时间不能太长否则会造成一致性问题<br>2. 热key打散逻辑需要细心的设计，整体的设计复杂度上升。<br>3. 一次消息分发，依旧要查询多个key，极端情况下会造成多次网络扇出请求，需要先查询session队列拿到离线消息流，然后在查询其上消息针对每个用户的状态，这将造成 N 次网络调用，因为redis的分布式结构，可能导致离线消息认列和消息状态不一个shard存储，这将造成多余的网络请求。                                                                                                                                                                                                                                                                                                                                                            |
| ==消息读写放大问题==<br>由于每个用户在同一个会话中看到的消息流是不同的。<br>1. 那么对于整个会话如果存储唯一一个离线消息列表，则只能存储基本信息，保证其共享性，这势必造成对于用户特化的信息要再去其他数据源读取一次，造成读延迟的增加，读时网络请求次数更多，这种情况称之为 **读放大**<br>2. 为了减少读放大问题，就需要把用户对消息的状态特化到离线消息列表中，这势必造成每个用户一份消息列表（消息的状态是对应到用户维度的），当有一条消息发送或者状态变更时，后台都需要有一个实时流计算的任务更新当前群聊中所有用户的消息列表，写入次数等价于群聊用户数，这种情况称之为 **写放大**<br><br>对于IM场景，读放大的存储成本小，一致性强，写复杂度低，读复杂度高。写放大的存储成本高，维护一致性成本高，写复杂，读简单。<br>单聊或者小群聊适合 使用写放大模式，来降低读的延迟，im场景往往写是异步的所以读延迟降低就能过满足用户体验。<br>大群聊/聊关室更加适合 读放大模式，因为大群聊/聊关室每次消息发送都相当于一次Ddos攻击，这造成极高的写成本以及写性能的下降，最终影响到读的性能（写没准备好自然也读不到），因此读放大模式，仅需要维护一份消息列表即可。减轻了写的负载。<br>**因此plato针对不同的业务场景结合读写放大策略的优势进行设计。**<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/90d5743bb4be84ba42890a6c9da952bf_MD5.jpeg)<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/eb501e9401fd730394cd5cb742f09ba5_MD5.jpeg)                                                                                                                                                                                                                                                                                                                                                | 有效的权衡了读与写的性能问题，使得极端情况下延迟能得到保证。                                                                                                                                                                         | 我们将一次消息下发所需要的网络调用次数已经尽可能的降低，但整体im每秒收发消息的次数依旧非常的恐怖。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ==好用的批处理==<br>如果对于消息下发场景，业务server聚合一个窗口，用来缓冲一段时间内对同一用户需要下发的消息，那么就可以有效减少需要进行的网络调用次数，通常窗口需要考虑时间与条数两个维度的限制，例如1s内，必须发送出去否则影响消息及时性，100条时必须发送，否则影响消息包总体大小。这一技术可以在业务server内存中自行实现，但这会导致业务server产生状态，违反无状态的设计模式，通常可以使用flink等流式计算平台达到相同效果,并且flink作为基础组件用来维护状态是符合云原生架构模式的。<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/1a83e27e3f7e030775e39691f4e7b639_MD5.jpeg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 尽可能的激少，网络调用的次数是降低延迟的关键，但还有好多其他优化策路我们会在后面遂一讲解，当前阶段为了讲明白设计缓存层，其最主要的瓶颈就在于读写缓存结构的次数，因为作为分布式缓存读写是要跨机器进行的。                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
### 3. 高吞吐 每秒下发 10M/s

| 方案                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 收益                                                                                                                                                                                                              | 代价                                                                                                                                                                                                 |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 关于session绑定方法的优化->==session升降级机制==<br>1. 定义专门的session Type：**super**，当接入层识别此session Type后直接存储sessionlD到conn对象的映射到本地内存中 <br>2. 当此session发生push操作时，直接通过sessionlD来查询此映射，直接push即可<br>3. 当业务方根据规则判断这是一个活跃大群时，在下一次push时，将session Type改为**super_upgrade**<br>4. 接入层im gateway感知后，则在这一次追历did分发时，将所有的conn对象都存储到<sessionlD，conn>对象的映射中<br>5. 并异步向业务server确认升级成功，回复ack，今后业务server分发此群聊消息时，sessiontype == super即可 <br>6. 对于降级情况sessiontype定义为super_demote即可，im gateway执行反操作即可。<br>7. 如果有部分机器失败，则下次还需要发送super_upgrade/super_demote的sesion type，进行重试，直到全部迁移完毕 <br>8. 此时用户拉取到会话时session Type已经变更，此时加入会话时接入层直接将其绑定到seesionlD到conn对象的映射中即可<br><br> | 1. 对于活联群或者超大聊室，此策略可以避免session的大key扇出查询，从而解决此性能瓶颈，整体吞吐量上涨，整个大群聊下发的瓶颈就被压到了im gateway单机的push上去，此时要么优化单机性能，要么就去水平扩展加机器得以解决，<br>2. 所以说，一个好的系统设计是能够让性能问题通过扩容来得到解决的，而坏的设计会造成系统性问题，扩容只会放大问题。<br>3. 很好的解决了小群聊广插机制的带宽浪费问题 | 1. 实现复杂度高，接入层与业务层需要进行频繁交互，这导教不同的业务方接入消息中台的成本增加，复杂度没有被屏蔽。<br>2. 整个升级过程中是单机操作的，存在局部失败的问题，如果出现某台机器失败的情况，要么重试，要么忽略，重试会导致性能下降，忽略会导致有群成员漏发消息，需要额外的事务机制来控制升级的一致性问题。<br>3. 相比于对小群聊广播造成的带宽浪费，此种方法引入的复杂度代价过高 |
| 关于session绑定方法的优化->==简单的广播/组播==<br>对于单聊全部时组播，对于群聊和即关室等场景全部时广插即可<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%B1%82/attachments/17c82d36c8b8e21b02e7be76057baba7_MD5.jpeg)                                                                                                                                                                                                                                                                                                                                                                                                     | 简单高效，非常完美                                                                                                                                                                                                       |                                                                                                                                                                                                    |
