
**目录**

- [背景介绍](#%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D)
- [基本分析](#%E5%9F%BA%E6%9C%AC%E5%88%86%E6%9E%90)
	- [基本流程](#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B)
	- [接口分析](#%E6%8E%A5%E5%8F%A3%E5%88%86%E6%9E%90)
- [设计目标](#%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87)
- [评估标准](#%E8%AF%84%E4%BC%B0%E6%A0%87%E5%87%86)
- [方案选型](#%E6%96%B9%E6%A1%88%E9%80%89%E5%9E%8B)
	- [1. 客户端如何选择网关IP地址？](#1.%20%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E7%BD%91%E5%85%B3IP%E5%9C%B0%E5%9D%80%EF%BC%9F)
	- [2. 如何设计**并发通信模型**来首发长连接消息？](#2.%20%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1**%E5%B9%B6%E5%8F%91%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%9E%8B**%E6%9D%A5%E9%A6%96%E5%8F%91%E9%95%BF%E8%BF%9E%E6%8E%A5%E6%B6%88%E6%81%AF%EF%BC%9F)
	- [3. 如何存储一个长连接的状态，才能既高效又节省内存](#3.%20%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E4%B8%80%E4%B8%AA%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%8A%B6%E6%80%81%EF%BC%8C%E6%89%8D%E8%83%BD%E6%97%A2%E9%AB%98%E6%95%88%E5%8F%88%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98)
- [思考题](#%E6%80%9D%E8%80%83%E9%A2%98)



## 背景介绍
Plato为保证消息的及时性需要使用tcp长链接与客户端进行通信(**节省DNS,握手等开销，并可主动push消息给客户端**），但长链接服务端需要一直维护连接状态。连接状态通常分为系统部分和应用部分，系统部分指的是socket的管理，应用部分指的得是连接过程中的uid/did/fd之间的映射关系，以及 clientID等信息的存储。
这些信息的生命周期是跟随一个长连接的创建而产生，长链接的断开而消亡极易变化，持久化存储除了用于数据分析，同时这些信息也是收发消息维度的访问频率，QPS极高，因此需要存储在内存中被使用。
这就导致整个长链接服务是一个有状态服务，难以运维和管理，业务需求的频紧上线会造成系统的重启更新，长链接势必会断开，客户端将有所感知，影响用户体验。因此，必须将长连接收发消息的功能和状态维护一个统一的服务，尽可能减少其重启的频率，保证其稳定性和收发消息的延迟。
这就是接入层的由来，而接入层的核心组件就是长连接网关，本节课程将设计**plato长连接网关**。

## 基本分析

### 基本流程
**当客户端初始化建立长链接时**
1. 向某个IP的长连接服务发送创建连接信令。 
2. 网关server解析信令得知其为创建连接信令。
3. 网关server，获得底层socket的FD，以及用户的uid/did，建立注册表。
4. 回复客户端连接建立成功。
---
**当客户端发送消息时**
1. 客户端发送上行消息信令。
2. 网关服务接收到消息，并解析信令为上行消息信令。
3. 根据clientiD和sessionlD进行路由，分配seqlD等状态更新逻辑。
4. 然后转发给业务层服务处理，确认业务层收到消息后立即回复客户端ACK。
---
**当业务处理后，将消息转发给接收客户端**
1. 业务根据sessionID定位到该会话的接收者的连接在哪一个网关服务上。
2. 然后将消息通过RPC交给网关服务，网关拿到数据后通过uid对应connlD，确定fd。
3. 然后根据fd找到对应的socket，将消息拼接固定消息头发送给接收方客户端。
---
**当连接断开时**
1. 心跳超时，连接断开/异常断开
2. 状态回收释放
### 接口分析
![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/9c831bb5b8c723a83310796986b11c9b_MD5.jpeg)
> 需要的信令
> 1. CreateConn创建连接
> 2. 发送上行消息 
> 3. 发送下行消息
> 4. 上行ACK回复消息 
> 5. 下行ACK确认消息
> 需要的RPC
> 6. 消息路由

## 设计目标
长链接接入层主要解决的问题就是**实现服务端主动及时地将消息发送给客户端的功能。而在这个过程中，会有非常多的技术挑战：**
1. 客户端如何选择网关IP地址？才能降低延迟，保证连接可靠，负载均衡？
2. 网关服务如何接收客户端的消息，获得最大的并发度获得消息的高吞吐，低延迟
3. 为了能使用长连接收发消息，需要维护哪些状态，如何使其占用更少的内存，单机承载更多的连接？
4. 业务层是怎么感知到连接在哪一个网关机器，并把消息分发下去的呢？如何降低网络请求的扇出？
5. 客户端进入地铁/切换基站/连接wifi等情况导致连接断开，如何能快速重连，而不影响用户体验？
6. 如何尽可能的减少长连接服务的连接/重启次数，做到永不宕机？
7. 长连接服务如向做限流/熔断/降级策略？实现对网关的过载保护，提高可靠性?
## 评估标准
1. 【可靠性】永不宕机，快速重连，快速重启，水平扩展，负载均衡（内存cpu/网络稳定）
2. 【低延时】收发消息在接入层的耗时p99不能超过5ms。
3. 【高吞吐】单机持有长链接数量以及活联连接每秒收发消息数。
4. 【上线频率】指长连接网关在一周时间内开发需求&bugfix导致上线的频率。
5. 【可拓展性】不影响用户的情况下增加网关机器或接入其他信令或业务，跨数据中心部等

## 方案选型
### 1. 客户端如何选择网关IP地址？

| 方案                                                                                                                                                                         | 收益                                                | 代价                                        |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- | ----------------------------------------- |
| 写死ip列表                                                                                                                                                                     | 实现简单                                              | 1. 毫无拓展性，更新拓展需要发版<br>2. 一旦ip被监控，没有兜底手段    |
| 使用httpDNS服务<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/0c5fd39d5e836c195d4a36f18d5b5049_MD5.jpeg) | 1. 可以水平扩展长连接网关<br>2. 精准调度 <br>3. 防止劫持 <br>4. 实时解析 | 1. 不能针对长连接来做精准调度<br>2. httpDNS本身也会带来可用性问题 |
| 自建一个http server作为ip config server<br>1. 通过一个域名+https协议访问ipconfig服务<br>2. 从中获得一批IP列表（减少请求&负载均衡&快速重连）<br>3. 客户端通过ip列表直接tcp连接长连接网关                                            | 1. 自建httpserver提供更高的可靠性 <br>2. 基于业务场景做智能调度策略      | 不能避免localDNS劫持问题                          |
| httpDNS + ip config<br>1. httpDNS解析获得正确的http server的公网ip地址<br>2. 然后通过此ip地址访问ip config server获得ip列表                                                                         | 1. 解决loacDNS问题<br>2. 实现长连接精准调度                    |                                           |
![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/6df13121d0c155d2374c1c4731c86338_MD5.jpeg)

### 2. 如何设计**并发通信模型**来收发长连接消息？
> C10K问题
> 但IM本质上是三方通信，需要考虑业务层的回调


| 方案                                                                                                                                                                                                                                                                                                                                     | 收益                                                                            | 代价                                                                                                                                 |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| 两个协程监听两个channel实现全双工+一个定时器协程<br>`goim`<br>1. 一个线程监听accept<br>2. 从accept socket返回创建连接消息<br>3. 服务端对这个fd创建两个协程分别负责收发消息<br>4. 每发送一个消息创建一个定时器对象并阻塞一个协程<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/25fa13246c08276a9a48d32e6dfd0cfa_MD5.jpeg)                       | 1. 实现简单，开发迅速<br>2. 基于出色go协程机制，也可以支持百万用户聊天(加机器）                                | 1. 内存占用高，难以突破单机C10M的瓶颈<br>2. 资源占用多将导致协程调度开销大，导致延迟升高<br>3. 一个协程占用4k内存，几万长连接时超过64G机器就有可能0OM<br>4. 每个下行消息都会创建一个定时器和协程，群聊，push场景很容易OOM |
| 一个协程使用select实现轮询阻塞<br>1. 一个conn对象创建后，分配一个协程阻塞在conn的read和send两个channel上 <br>2. 在一个select语句上轮询两个channel,谁有消息到来就去处理谁的逻辑<br>3. 业务回调时，开辟一个协程通过注册表找到send channel,交给连接处理协程发送消息<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/22dda93c8bf9b1360cea6a048d309e0c_MD5.jpeg) | 1. 节省了一个协程的开销，内存占用减少三分之一<br>2. 协程数量减半，runtime调度开销减少，延迟有所提高                    | 1. 同一时间只能接收或发送消息，群聊场最延迟升高 <br>2. 协程的阻塞和唤醒在消息收发场景下依旧是瓶颈 <br>3. 依旧没有解决下行消息时定时器和协程内存问题                                                |
| goroutine pool<br>1. 一个协程阻塞监听socket的read函数 <br>2. 有信令到达后解析并处理<br>3. 业务层回调时，也是取goroutine pool取一个用来处理向socket send消息。<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/173b99312f160bc1ea494a50edcac5d9_MD5.jpeg)                                                      | 1. 可以全双工收发消息<br>2. 业务层回调函数，使用协程池化技术，减少了协程的调度开销<br>3. 限制了协程资源的上界，避免协程分配过多导致OOM | 1. 还是会有一个协程被阻塞<br>2. 需要维护conn的索引，才能让从协程池中获得的协程找到socket                                                                             |
| reactor+goroutine pool<br>> 端到端设计原则 <br>1. 通过epoll系统调用，将收发消息完全事件化<br>2. 当epoll读来临时，从goroutine pool中拿到消息并解析后转发 <br>3. 当业务层回调的时候，直接从goroutine pool中<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/74993394adb33f21eafea9160cdb52cd_MD5.jpeg)                       | 收发消息无协程阻塞，减少了调度开销与内存占用                                                        |                                                                                                                                    |
### 3. 如何存储一个长连接的状态，才能既高效又节省内存

| 方案                                                                                                                                                                                                                                                                                                                                                                                                                   | 收益                                                                  | 代价                                                                                                                                |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| 状态映射表<br>1. 创建connect对象存储与连接相关的状态，fd/clientID/飞行队列/定时器<br>2. 维护uid/did到connect对象所在机器endpoint倒排<br>3. 维护sessionID到connID的MAP，以及connID和connect对象的Map <br>4. 中心化存储session对象，uid/did，seqlD，业务状态等信息。<br>5. 业务层回调发送下行消息时，可以通过sessionlD找到uid，再通过uid找到connect<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/bbc30c13f168090da4e9e63c6723ab52_MD5.jpeg) | 1. 点查性能相对较高 <br>2. 实现相对简单                                           | 1. 飞行队列和定时器占用大量内存，成为瓶颈<br>2. 定时器占用内存空间，同时也要占用一个协程用于消息重发 <br>3. session是中心化存储的，需要进行网络调用访问<br>4. 群聊场景从uid反查conn所在endpoint的操作会导致网络扇出 |
| 1. 飞行队列使用redis list结构存储<br>2. 使用时间轮算法代替golang原生的四叉堆实现<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/51e0a99e9e50b88ca7eb8b0062b7d2ff_MD5.jpeg)                                                                                                                                                                                                 | 1. 减少了网关的内存消耗<br>2. 使得协程规模从线性变为常数                                   | 内存还是要维护时间轮数据结构                                                                                                                    |
| 拆分微服务state server<br>1. 网关server仅维护connlD到fd的map映射<br>2. connect对象中定时器，飞行队列等状态交给完全独立的state server维护，与网关server之间通过RPC进行通信<br>3. 业务层将消息发送给state server由其控制收发逻辑<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/77530cc8abf320efa432b6253f823f5f_MD5.jpeg)                                                                                          | 1. 极大的节约了网关server的内存 <br>2. 同时提高了网关server的可靠性<br>3. 降低了网关server重启次数 | 1. 多了一次RPC网络调用<br>2. 群聊场景下，会造成消息风暴<br>3. 业务层通过sessionD查到uidlist，再跳到conn所在的机器上发送消息，依旧有高扇出问题<br>4. State Server也需要考虑分布式的设计，复杂度上升    |
| 容器绑定state server与gateway serve<br>1. State与Gateway一一对应，在一组容器内两个业务进程 <br>2. 进程之间通过domain socket进行通信                                                                                                                                                                                                                                                                                                                   | 减少了state与gateway的网络开销                                               | 1. 造成服务之间相互依赖，扩展性降低<br>2. 两个进程之间会共享os进而资源共享，导致可靠性降低                                                                               |

## 思考题
1. 如何解决业务层回调长连接网关时的消息扇出问题？
2. 如何保证长连接本身的可靠性，降低其断线的影响？
3. 如何提高长连接服务故障恢复的速度？
4. 长连接服务如何做限流/熔断/降级策略？