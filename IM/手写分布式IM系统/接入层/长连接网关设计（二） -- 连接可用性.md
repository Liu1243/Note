## 方案选型
### 4. 业务层是怎么感知到连接到哪一个网关机器，并把消息分发下去的呢？如何降低网络请求的扇出？

| 方案                                                                                                                                                                                                                                                                                                                                                                                                                   | 收益                                                                                                    | 代价                                                                                                                       |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| ==全扇出==<br>1. **利用redis的pub/sub功能，订阅所有的长连server(MQ也可以）** <br>2. 业务服务将消息push到消息总线<br>3. 消息总线会将消息分发给所有的长连服务，包含session和uid/did信息<br>4. 所有的长连server会检查自己本地是否存在此session的socket(排除发送者） <br>5. 如果有则直接发送，否则忽略<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/afc076fde73ef0964e574dcdb2433a7e_MD5.jpeg)                                                 | 1. 业务侧分发逻辑简单，实现简单<br>2. 不需要维护全局状态，成为性能瓶颈<br>3. 群聊/push等场景无效请求量较少，整体吞吐量高                               | 1. 对于c2c场景或者小群聊会存在大量无效分<br>发情况。<br>2. 大量无效网络调用、占用带宽<br>3. 网络请求，会消耗长连网关服务的协程资<br>源，致使延迟增大，甚至OOM。<br>4. 跨数据中心的广域push，直接完蛋。 |
| ==一致性hash==<br>1. 基于服务发现，业务服务感知长连网关的节点数目，实现一致性hash<br>2. 创建连接时也适用相同的hash，算法保证一个用户的长连接必定在某台机器上<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/95e278fefbe296c0f1bd4a624e330b2b_MD5.jpeg)                                                                                                                                                         | 1. 性能足够好<br>2. 不会破坏分片的扩展性，可伸缩扩展                                                                       | 1. 使得ip config server的调度能力受到约束 <br>2. 群聊等场景作用不大                                                                          |
| ==映射路由表 服务 -- 独立出路由服务==<br>> 由专门的路由服务集群实现路由数据的更新和存储<br>1. 维护sessionID到uid list的映射 <br>2. 维护uid到did list的映射<br>3. 维护did到connID list的映射 <br>4. 维护connlD到endpoint到映射<br>5. 创建连接时路由服务直接维护一个sessionlD到endpont的倒排结构 <br>6. 业务服务去查询路由服务获得endpoint然后调用RPC分发消息<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/c834735803edb47e486e1daf5013e1c4_MD5.jpeg) | 1. 精准传输，没有浪费网络带宽 <br>2. 适用于c2c场景的消息分发 <br>3. 实现简单，内存操作性能好 <br>4. 集群隔离，增强可用性<br>5. 屏蔽底层存储细节，适用于多业务方接入  | 1. 群聊/push场景产生大量扇出时，查询路由表没有意义，造成浪费<br>2. 多一跳网络请求，增加网络通信复杂度 <br>3. 跨数据中心的广域push，直接完蛋。 <br>4. 维护全局存储成本过高                   |
| ==State server负责路由==<br>1. 有state server维护路由信息<br>2. 业务服务仅需要将seesionlD告知state即可<br>3. state服务会自行找的具体的长连网关服务，进行消息分发<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/68f531a32a5238d5c37a4e478d4a699c_MD5.jpeg)                                                                                                                                    | 减少网络调用次数，延迟降低                                                                                         | 1. State变的会更加复杂，依赖更多，可用性难以保证<br>2. 无法应对消息风暴<br>3. 跨数据中心的广域push，直接完蛋。                                                     |
| ==组合策略==<br>1. 对于c2c聊天或者非活跃群，采用消息路由的方式<br>2. 对于大规模活跃群，采用全扇出分发模式                                                                                                                                                                                                                                                                                                                                                      | 综合考虑，做到效果最优化                                                                                          | 实现变得复杂                                                                                                                   |
| ==基于地理位置==<br>基于IP等地理位置，将相近的用户放在一个同一个网关server上                                                                                                                                                                                                                                                                                                                                                                       | 1. 对于近邻社交等场景效果明显 <br>2. 适用于o20，同城交友场景                                                                 | 1. 策略单一，限制过大 <br>2. 实现复杂                                                                                                 |
| ==基于社交关系的图计算==<br>1. 根据社交关系网络，以会话活跃度为权重，计算一个最佳分配策略 <br>2. 离线或近线的将活跃聊天的用户尽量放在一个长连网关上<br>3. 并维护一个session到endpoint的倒排索引，方便快速查询定位                                                                                                                                                                                                                                                                                        | 1. 基于社交网络做到调度上的量最优化<br>2. 最小化分发代价，使得性能达到最优化 <br>3. 基于社交关系的连接调度可以带来非常多的细粒度优化<br>4. 较好的应对跨数据中心的广域push请求 | 实现变得复杂                                                                                                                   |
| ==基于弱网感知（附近建立数据中心）==                                                                                                                                                                                                                                                                                                                                                                                                 | 实现简单，效果明显                                                                                             | 建设数据中心成本过高                                                                                                               |
![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/38ab3aaa237f26021481593de5a7d42f_MD5.jpeg)

### 5. 客户进入地铁，切换基站，链接wifi等情况导致连接断开，如何快速重连，而不影响用户体验？
> 为什么要应用层自己维护心跳？

![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/3fd33ad7feddee2f97b3764bccc7681f_MD5.jpeg)

| 方案                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 收益                                                  | 代价                                                                                  |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------- | ----------------------------------------------------------------------------------- |
| ==租约机制==<br>客户端与服务端建立长连接心跳（端到端设计）<br>1. 由客户端发起心跳，降低服务端的性能开销<br>2. 服务端维护一入连接超时计时器，一旦计时超时，主动断开连接回收资源<br>3. 如果在超时之前，客户端的心跳达到，则重置计时器超时时间 <br>4. 每次收发消息都可以当作一次心跳，来减少心跳次数<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/260623f422066bad30b025cdabb8243f_MD5.jpeg)                                                                                                                                       | 1. 保证连接的端到端可靠性<br>2. 实现简单，逻辑清晰<br>3. 避免被运营商截断空连接的风险 | 1. 弱网环境下，连接极易断开，只要断开就会释放资源，造成浪费<br>2. 有运营商路由器的空闲连接限制，心跳时间不受控制<br>3. 心跳会造成流量潮汐，压垮服务  |
| ==断线重连==<br>1. 当连接断开，或者心跳超时后，启动一个重连计时器 <br>2. 当重连计时器超时时，才真正的回收状态资源。<br>3. 否则当客户端在重连计时到期前重新连接时，创建连接，State server会复用之前的状态，仅替换一下session到endpoint等路由表信息即可<br>4. 客户端如果断线，则重连当前IP的gateway。<br>5. 如果重连无效，再从ip config中选择其他ip进行重连。 <br>6. 重连时，可以随机一个等待时间保证流量打散<br>7. 如果在同一gateway重连可复用全部状态，如果在另一个gteway重连则可复用全局张图。<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/f88695fedefc1e05eb9907dad755bc2a_MD5.jpeg) | 应对一定的弱网环境，避免频整断线重连，资源释放与创建的开销                       | 1. 判断是否是弱网的条件是一个固定值，不能很好的适应变化场景<br>2. 暂时缓解，流量潮汐问题<br>3. 无法跨网关机器做状态复用。               |
| ==主备连接==<br>1. 为降低用户切换连接的感知，可以为客户端维护两条长链 <br>2. 一个用来正常收发消息，另一个用户备用(push等场景)<br>3. 当im占用长链断开时，可以通过push长连进行下行消息的下发 <br>4. 上行消息退化为http的post请求即可<br>![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/94082c95bb335da5cfff55c5a5c540e2_MD5.jpeg)                                                                                                                                                              | 1. 减少对用户体验的影响<br>2. 在长连不可用的情况下收发消息                  | 1. 实现复杂，需要单独实现一套逻辑 <br>2. 需要和push长连打通逻辑<br>3. 如果app没有占用push长连则要自己维护一个长连进行backup成本过高 |
| ==协议升降级==<br>1. 正确识别出弱网环境时，将协议改为QUIC/SPDY<br>2. TCP->QUIC->HTTP的降级链路 <br>3. Gateway仅解析长连接的固定消息头<br>4. 将剩余字节数组交给state服务进行解析，完全由state server控制                                                                                                                                                                                                                                                                                                                             | 1. 较好的应对弱网问题<br>2. 改进了TCP的诸多弊端，性能更好                 | 1. 实现复杂，坑比较多<br>2. QUIC基于UDP，UDP协议发展不够完善<br>3. QUIC协议没有非对称加密，安全性会受到影响               |
![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/9c96bc818efbcff193e01b5cf357e7f3_MD5.jpeg)

### 6. 如何尽可能的减少长连接服务的崩溃，重启次数，做到永不宕机？

| 方案                                                                                                                                                                                                                                                                                                                                                                             | 收益                                                                        | 代价                                                                                                                                         |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| ==运行时隔离==<br>gateway和state拆分为两个进程，运行时隔离                                                                                                                                                                                                                                                                                                                                        | 降低重启次数，可灵活部署，在各个层次均可做到隔离                                                  | 1. 会增加实现复杂度<br>2. 部署在同一机器上会减少网络调用次数，部署到多个机器上可靠性会增强                                                                                         |
| ==共享内存==<br>1. State server可以将自身的状态保存到共享内存中<br>2. 以便于崩溃后，可以理解从共享内存中恢复                                                                                                                                                                                                                                                                                                          | 减少崩渍带来的影响                                                                 | 实现变得复杂                                                                                                                                     |
| ==长连接平滑启动==<br>当长连接服务更新代码时，如何保证其平滑启动呢？<br>1. 创建新进程，并禁止accept连接创建事件。 <br>2. 注册新进程，接收accept连接创建事件。<br>3. 老进程创建一个domain socket并连接新老进程<br>4. 新进程连接后回复ack，代表连接 <br>5. 老进程收到ack后开始同步历史连接<br>6. 先将映射状态序列化后同步过去，紧跟着FD也同步，同步时对scoekt上锁 <br>7. Domain socket的特点，会将fd转换为socket对象<br>8. 新进程建立新的socket与connlD的映射，然后注册到自身的epoll对象上，<br>然后回复老进程ack<br>9. 老进程接收到ack后，从自身epoll中删除该socket并释放内存资源 | 1. 保证重启长连接服务客户端完全无感知，用户体验达到最优<br>2. 避免了大规模客户端重新连接的流量潮汐                    | 1. 如果单机连接过多，会导致迁移时间过长 <br>2. 实现复杂，容易出错，需要正确权衡其ROI<br>3. 仅用于单机实现的平滑重启<br>4. 如果扩容，老连接无法迁移，保证均衡<br>5. 将长连网关进行状态分离后，此方案ROI不高                   |
| ==优雅关闭==<br>1. 接受进程信号，或者web hook触发后或者ops server的rpc<br>2. 先禁止在此gateway上创建新连接，然后逐一发送给客户端下线信令<br>3. 客户端与其他网关建立连接后，回复确认下线，并立即断开连接 <br>4. 服务端接到确认后，回收对应状态资源                                                                                                                                                                                                                        | 1. 对用户体验无损，方便网关集群水平扩展 <br>2. 是各种运维操作的基本操作，是很必要的                           | 1. 与客户端交互比较复杂<br>2. 在优雅关闭状态下，各种bad case难以处理，可能导致，永远无法关闭或者下线server时间极长                                                                      |
| ==跨机器扩缩容迁移长连接==<br>1. 当增加一台网关机器时，state server会划分出将要迁移的connlD有哪些 <br>2. 然后发起RPC逐一控制迁移，先用老网关上的连接给客户端发送建立backup连接信令，并告知新网关的ip地址<br>3 .然后客户端与新网关建立连接，并回复ack给网关server<br>4. state知道后通知老网关关闭连接，连接关闭成功后，客户端回复ack <br>5. 接到确认消息后，state切换映射状态，使用新网关的连接进行通信<br>                                                                                                                          | 1. 跨机器，跨数据中心实现无损体验的平滑迁移方案<br>2. 在线迁移，最小粒度控制锁，性能良好 <br>3. 较好的解决长连网关的可伸缩性问题 | 1. 灵感来自于redis，但长连接迁移，其中复杂度较高，很容易出现bad case，难以排查问题，必须存在人工手动干预操作，并且可以快速恢复原状，避免影响业务运行<br>2. 依赖客户端实现，逻辑较为复杂<br>3. State server做全局操作比较复杂，破坏设计规范 |
| ==拆解出ops运维server角色==<br>1. 维护一个专门用来接收线上指标，分析业务状态并作出运维决策的服务 <br>2. 此服务根据业务指标，判断业务状态，根据业务状态，产生运维任务 <br>3. 执行运维任务，对线上集群的各种操作做到自动化处理，并可人工干预                                                                                                                                                                                                                                        | 1. 后期规模化后，方便运维操作和成本管理<br>2. 自动化运维操作，快速感知，止损，修复故障，可以提升整体系统可用性              | 设计复杂，小规模场景根本没必要，手写py脚本人工运维即可                                                                                                               |
![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/695850dd5f68499eeafb2efcc9fb5ae7_MD5.jpeg)

### 7. 长连接如何做限流，熔断，降级策略？实现对网关的过载保护，提高可靠性？

| 方案                                                                                                                                                                         | 收益                                                               | 代价                                                      |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ------------------------------------------------------- |
| ==基于服务发现==<br>> **硬核课堂：如何设计一个分布式限流系统**<br>1. 基于服务发现做config的分发<br>2. 将分布式限流状态分发给单机 <br>3. 单机使用令牌桶等算法实现限流                                                                    | 1. 防止流量突增压跨流量<br>2. 对连接数和消息数，实现全局状态管理                            | 不能动态的调整阅值，应对多种复杂场景的漏限和误限                                |
| ==负反馈调节==<br>1. 基于近线采样收集的日志，感知全局状态的变化，然后由算法模型决策出当前最佳参数。 <br>2. push给单机网关server，来进行限流配置                                                                                     | 1. 动态调整菌值，优化漏限和误限情况<br>2. 负反馈机制实现自动化，实时的运维操作                     | 实现复杂，收益需要进一步权衡讨论                                        |
| ==多目标融合==<br>1. 消息收发的失败率 <br>2. 连接断开重连次数 <br>3. 心跳包的RTT<br>4. 内存/IO/CPU使用率<br>5. 定时器数目&飞行队列总消息数<br>6. 飞行队列百分位长度 <br>7. 协程池中工作协程数<br>8. socket等内核监控指标 <br>9. 打开客户端30s内网连成功率 | 1. 综合考虑多个优化目标，最终系统平稳运行 <br>2. 通过iP config server将最终ip排序结果发送给客户端。 | 1. 实现复杂，难以调试和解 <br>2. 较难衡量哪一种指标更有效果 <br>3. 较难得到一个好的融合公式 |
![](IM/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8FIM%E7%B3%BB%E7%BB%9F/%E6%8E%A5%E5%85%A5%E5%B1%82/attachments/2abc35a3d6323dc45adc7d9c3043631a_MD5.jpeg)
