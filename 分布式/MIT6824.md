**目录**

- [01 Introduction](#01%20Introduction)
	- [分布式底层基础架构：](#%E5%88%86%E5%B8%83%E5%BC%8F%E5%BA%95%E5%B1%82%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%EF%BC%9A)
	- [MapReduce](#MapReduce)
- [02 RPC and Threads](#02%20RPC%20and%20Threads)
- [03 GFS](#03%20GFS)
	- [GFS的目标](#GFS%E7%9A%84%E7%9B%AE%E6%A0%87)
	- [架构](#%E6%9E%B6%E6%9E%84)
	- [master节点](#master%E8%8A%82%E7%82%B9)
	- [读文件](#%E8%AF%BB%E6%96%87%E4%BB%B6)
	- [租约](#%E7%A7%9F%E7%BA%A6)
	- [写文件](#%E5%86%99%E6%96%87%E4%BB%B6)
	- [一致性模型](#%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B)
	- [快照](#%E5%BF%AB%E7%85%A7)
	- [数据完整性](#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7)
	- [总结](#%E6%80%BB%E7%BB%93)
- [04 主从复制（Primary/Backup Replication）](#04%20%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%EF%BC%88Primary/Backup%20Replication%EF%BC%89)
	- [复制的方式](#%E5%A4%8D%E5%88%B6%E7%9A%84%E6%96%B9%E5%BC%8F)
	- [复制的挑战](#%E5%A4%8D%E5%88%B6%E7%9A%84%E6%8C%91%E6%88%98)
	- [VMware FT论文总结](#VMware%20FT%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93)
	- [确定性重放（Deterministic replay）](#%E7%A1%AE%E5%AE%9A%E6%80%A7%E9%87%8D%E6%94%BE%EF%BC%88Deterministic%20replay%EF%BC%89)
	- [FT协议](#FT%E5%8D%8F%E8%AE%AE)
	- [发现与处理故障](#%E5%8F%91%E7%8E%B0%E4%B8%8E%E5%A4%84%E7%90%86%E6%95%85%E9%9A%9C)
	- [FT的实现细节](#FT%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82)
	- [替代方案](#%E6%9B%BF%E4%BB%A3%E6%96%B9%E6%A1%88)
	- [FAQ](#FAQ)

## 01 Introduction
### 分布式底层基础架构：
- Storage，存储基础架构，比如键值服务器、文件系统等
- Computation，计算框架，用来编排或构件分布式应用程序，比如经典的mapreduce
- Comminication，分布式系统逃不开网络通信问题，比如后续会讨论的RPC(远程过程调用)
RPC提供的语义：
- **at most once：最多一次**
- **exactly once：恰好一次**
- **at least once：至少一次**

分布式系统的3个重要特性：
**fault tolerance 容错性：**
包括可用性，用p999指标度量，依赖replication
可恢复性，依赖logging 或者 transaction 以及 durable storage

**consistency 一致性**：
强一致性、最终一致性

**performance 性能**：
throughout 吞吐量
latency 低延迟，避免尾部延迟（一台机器执行慢，拖慢整个请求响应）

==需要做到一致性与性能的权衡==

### MapReduce
具体参考：[mp.weixin.qq.com/s/I0PBo\_O8sl18O5cgMvQPYA](https://mp.weixin.qq.com/s/I0PBo_O8sl18O5cgMvQPYA)
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/d4fa0b3b50f4a3ecd6d958ed899ba97a_MD5.jpeg)
**工作流：**
1. 将输入文件分成M个小文件，集群启动MapReduce实例，包括一个Master和多个Worker
2. Master分配任务，将Map任务分配给Worker节点
3. Map Worker读取文件，执行用户定义的map函数，输出key、value，缓存在内存中
4. 内存中的key、value通过partitioning function分为R个regions，写入磁盘，将文件地址回传给Master，Master将地址传给Reduce Worker
5. Reduce Worker收到文件位置信息，通过RPC获取数据，根据key排序，将同一key的数据分组聚合
6. Reduce Worker将分组后的值执行用户自定义的reduce函数，输出追加到所属分区的输出文件中
7. 当所有Map、Reduce任务完成后，Master向用户程序返回结果
**实例：**
词频统计

**容错性**：
Worker故障：Master向Worker发送心跳包，指定时间内没返回，标记为Worker失效，将任务分配给其他Worker
Master故障：终止子整个MapReduce任务，重新执行

**性能**：
**网络带宽匮乏**：尽量将Map任务分配给包含输入数据的机器上执行，如果找不到，就近分配
**“落伍者（Stragglers）”**：单点机器执行慢，通过备用任务（backup tasks）处理，Master调度其他进程同时处理该任务，其中一个执行完成， 就标记为任务完成。

**Combiner函数**：
在某些情况下，Map函数产生的中间key值重复数据多，用户可以自定义一个Combiner函数，先将本地的记录进行一次合并，然后再将合并的结果通过网络发送出去。
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/d80f085916fe694320c83ab33b53f2b0_MD5.jpeg)
==Combiner函数通常和Reduce函数的代码相同，使用Combiner的好处是减少网络的通信量==
为什么默认禁用Combiner？
Combiner与Reducer一个是本地聚合，一个是全局聚合，逻辑会有偏差，会影响到执行结果的正确性。

## 02 RPC and Threads
参考：[微信公众平台](https://mp.weixin.qq.com/s?__biz=MzIwODA2NjIxOA==&mid=2247484193&idx=1&sn=693e0ff4bfcc6e02dea10ed9d639b41b&chksm=970980e4a07e09f2647de63ed0bf3be98d9032a3797033af3872c692d2373f98627a63f30e22&cur_album_id=1751707148520112128&scene=189#wechat_redirect)
Go通过channel、sync.Cond、WaitGroup解决线程问题。
GO RPC实现了==At Most Once==语义，如果没有得到响应，只会返回一个错误。客户端可以重试一个失败请求，服务端需要处理重复请求的情况：
- 如何保证多个客户端的 ID 是唯一的？可以带上客户端 ID，类似于：`<client_id, seq>`(和 Raft 客户端交互那部分内容对应上了！)
- 但我们不可能无期限地保存所有的请求 ID，保存多长时间？可以在客户端的请求中包含一个额外的标识符 X，告诉服务端删除 X 之前的所有请求 ID 是安全的
- 当原始请求还在执行时，如何处理重复的请求？可以等待它完成，也可以直接忽略新的请求。
- 为了避免服务器宕机，ID 信息还需要写入到磁盘，也许还要跨机器多副本存储。

## 03 GFS
参考：[mp.weixin.qq.com/s?\_\_biz=MzIwODA2NjIxOA==&mid=2247484205&idx=1&sn=ff45aeaeac29b79a6fad53f912a16933&chksm=970980e8a07e09feebe17c4cde354cd3caf1432ea0897ba4db7d08f05ffe245ec46773e36fb9&cur\_album\_id=1751707148520112128&scene=189#wechat\_redirect](https://mp.weixin.qq.com/s?__biz=MzIwODA2NjIxOA==&mid=2247484205&idx=1&sn=ff45aeaeac29b79a6fad53f912a16933&chksm=970980e8a07e09feebe17c4cde354cd3caf1432ea0897ba4db7d08f05ffe245ec46773e36fb9&cur_album_id=1751707148520112128&scene=189#wechat_redirect)

### GFS的目标
- 大型：大容量，需要存放大量的数据集；
- 性能：自动分片(Auto-Sharding)；
- 全局：不只是为一个应用而定制，适用于各种不同的应用；
- 容错：自动容错，不希望每次服务器出了故障，都要手动去修复；
### 架构
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/61e85453876b90e37c380a2d7298b277_MD5.jpeg)
GFS集群包含一个master和多个chunkserver，并且有多个client进行交互。
**chunk**：存储在GFS中的文件会被分为多个chunk，每个chunk大小为64M，master会分配一个不可变、全局唯一的64位ID（chunk handle）；默认一个chunk有三个副本，在不同的chunk server上。
**master**：维护文件系统的metadata；负责chunk的迁移、reblancing和垃圾回收；通过心跳与chunkserver通信
**client**：向master询问文件metadata，去对应的chunkserver获取数据
**chunkserver**：存储chunk，chunkserver月client不会缓存chunk数据，防止数据的不一致

### master节点
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/b50ae88811249a943c7411edcf0a6181_MD5.jpeg)
GFS只有一个master；master存储3种metadata，标记nv（non-volatile）需要在写入的同时存到磁盘，标记为v的数据master启动后查询chunkserver集群：
- namespace和文件名 nv
- 文件名-》chunk的映射 nv
- chunk-〉版本号（nv）、chunkservers（v）、primary（v）、租约（v）
master会在本地磁盘存储log，而不是存在数据库；相比于数据库操作，追加log很高效，并且通过在log中插入checkpoint点，重建状态也会很快

### 读文件
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/f2a393db68896a61123940e556135be2_MD5.jpeg)
- client 将 文件名+offset 转为文件名+ `chunk index`，向 master 发起请求；
- master 在内存中的 metadata 查询对应 chunk 所在的 `chunk handle` + `chunk locations` 并返回给 client；
- client 将 master 返回给它的信息缓存起来，用文件名 + `chunk index` 作为 key；(**注意：client 只缓存 metadata，不缓存 chunk 数据**)
- client 会选择网络上最近的 chunkserver 通信(Google 的数据中心中，IP 地址是连续的，所以可以从 IP 地址差异判断网络位置的远近)，并通过 `chunk handle` + `chunk locations` 来读取数据；
### 租约
如果每次写都要请求master，那么master会成为性能瓶颈；master首先会找到拥有该chunk的chunkserver，为其中一个chunkserver授予==租约==，拥有租约的chunkserver称为==Primary==，其他称为==Secondary==
- master 会增加版本号，并将版本号写入磁盘，然后 master 会向 `Primary` 和`Secondary` 副本对应的服务器发送消息并告诉它们，谁是 `Primary`，谁是 `Secondary`，最新的版本号是什么；
- 在租约有效期内，对该 chunk 的写操作都由 `Primary` 负责；
- 租约的有效期一般为 60 秒，租约到期后 master 可以自由地授予租约；
- master 可能会在租约到期前撤销租约(例如：重命名文件时)；
- 在写 chunk 时，`Primary` 也可以请求延长租约有效期，直至整个写完 chunk；
### 写文件
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/ff50f1b2b1346b8de435f1288bc43583_MD5.jpeg)
1. client 向 master 询问 `Primary` 和 `Secondary`。如果没有 chunkserver 持有租约，master 选择一个授予租约；
2. master 返回 `Primary` 和 `Secondary` 的信息，client 缓存这些信息，只有当 `Primary` 不可达或者**租约过期**才再次联系 master；
3. client 将追加的记录发送到**每一个 chunkserver(不仅仅是 `Primary`)**，chunkserver 先将数据写到 LRU 缓存中(不是硬盘！)；
4. 一旦 client 确认每个 chunkserver 都收到数据，client 向 `Primary` 发送写请求，`Primary` 可能会收到多个连续的写请求，会先将这些操作的顺序写入本地；
5. `Primary` 做完写请求后，将写请求和顺序转发给所有的 `Secondary`，让他们以同样的顺序写数据；
6. `Secondary` 完成后应答 `Primary`；
7. `Primary` 应答 client 成功或失败。如果出现失败，client 会重试，但在重试整个写之前，会先重复步骤 3-7；
### 一致性模型
GFS是一个弱一致性模型，并不保证一个chunk的所有副本是相同的。如果一个写失败，client会重试
- 对于写：可能有部分副本成功，而另一部分失败，副本就会不一致。
- 对于 `record append`：也会重试，但是不是在原来的 offset 上重试，而是在失败的记录后面重试，这样 `record append` 留下的不一致是永久的不一致，并且会让副本包含重复的数据。

### 快照
GFS通过snapshot创建一个文件或者目录树的备份，勇于备份文件或者创建checkpoint。GFS使用写时复制COW来写快照

### 数据完整性
每一个chunkserver使用checksum来检验存储数据的完整性
每个chunk以64kb的块进行划分，每一个块对应一个32位的checksum

### 总结
GFS最严重的局限性是只有一个master节点
- 随着 GFS 的应用越来越多，文件也越来越多，最后 master 会耗尽内存来存储 metadata；你可以增加内存，但单台计算机的内存始终有上限；
- master 节点要承载数千个 client 的请求，master 节点的 CPU 每秒只能处理数百个请求，尤其是还要将部分数据写入磁盘——client 的数量会超过单个 master 的能力；
- 弱一致性会导致应用程序很难处理 GFS 奇怪的语义；
- 最后一个问题，master 的故障切换不是自动的，需要人工干预来处理已经永久故障的 master 节点，并更换新的服务器，这需要几十分钟甚至更长的时间来处理。对于某些应用程序来说，这个时间太长了。

## 04 主从复制（Primary/Backup Replication）
这一课讨论关于容错(Fault-Tolerance)和复制(Replication)的问题，主要研究 VMware FT 的论文 —— **The Design of a Practical System for Fault-Tolerant Virtual Machines**

### 复制的方式
- 状态转移（State Transfer）：Primary将自己所有的状态拷贝发送给Backup备机，一般是增量备份
- 复制状态机（Replicated State Machine）：client发送操作给主机，主机按照顺序发送到备机

VMware FT使用了复制状态机的方法。
> 状态转移是内存，复制状态机是客户端的操作。倾向于使用复制状态机的原因是，外部操作比服务的内存状态要小。

### 复制的挑战
- 我们要复制哪些状态？
- 主机必须等待备机备份完吗？
- 什么时候切换到备机？
- 切换时能否看到异常情况？
- 如果有个副本故障了，我们需要重新添加一个新的副本，这可能是一个代价很高的行为，因为副本可能非常大，如何提升添加新副本的速度？

### VMware FT论文总结
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/fa48e631d48d5121c88197949d17709c_MD5.jpeg)
Primary VM为主机，Backup VM为备机，虚拟机的虚拟磁盘在共享存储上。
所有的输入输入到主机，然后通过Logging channel转发到备机。
==两台虚拟机都会执行输入，但是只有主机的输出会返回给客户端，备机的输出被管理程序丢弃==

### 确定性重放（Deterministic replay）
不确定性事件比如虚拟中断，会让主机与备机的运行结果不一致。
VMware通过==确定性重放==捕获所有的输入和可能的不确定输入，写入到日志文件记录下来。

### FT协议
VMware通过确定性重放产生日志，但不将日志写入磁盘，而是用过Logging channel发送给备机，备机实时重放日志项。
为了容错，需要在Logging channel上实现容错协议，要求：
**输出要求**：如果备机在主机故障后接管，备机将以和主机一致的方式运行
**输出规则**：主机直到备机接受并确定了和输出相关的日志，才发送输出给外界
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/09f0792b520d4ad7dd323f2e0cfe8188_MD5.jpeg)
==每个复制系统都有这个问题：在某个时间点，主机必须停下来等待备机，这会影响性能==

### 发现与处理故障
通过==udp心跳包==和==监控Logging channel上的流量==相结合来检测，如果心跳包超时或logging channel流量停止则表明故障
如果备机故障，主机停止向logging channel发送日志

> 备机故障后如何追上主机？
> VMware通过VMotion，能够在最小程度上终端虚拟机的运行， 克隆一个虚拟机

如果主机故障，备机必须重放完所有日志，然后备机接管主机
为了确保一次只有一个虚拟机称为主机，避免脑裂，Vmware在共享存储上会执行一个原子`test-and-set`锁指令，只有一个机器返回成功

### FT的实现细节
VMware VMotion最小化中断，将运行中的虚拟机从一台服务器迁移到另一台服务器，这种克隆操作打断主机的时间不超过1秒
**管理Logging Channel**
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/789f74b24fc2890acab629367c4882e9_MD5.jpeg)
通常，主机日志缓冲充满的原因：
- 带宽太小，**建议日志通道带宽 1Gbit/s**；
- 备机执行速度太慢，从而消费日志太慢时，主机的日志缓冲也可能会被填满；
VMware FT中实现了一种机制，当备机远远落后时（落后1s），可以减缓主机的执行速度。通常只会在系统处于极端压力的情况下发生。

### 替代方案
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/d77de288a6f9ba3d8f766eac82421918_MD5.jpeg)
==共享磁盘和非共享磁盘==：VMware FT中使用了一个主备机都能访问的共享存储。替代方案是使用单独的虚拟磁盘，优点是：存储不能同时被主备机访问，共享存储太贵；缺点：需要同步磁盘状态
==备机上执行磁盘读取==：目前实现中，备机不会从磁盘中读取，因为磁盘操作被认为是一种输入。替代方案是备机可以进行磁盘读取，当大量磁盘读的工作负载，这种方式可以减少日志通道的流量，缺点：
- 可能会减慢备机的执行速度，因为备机必须执行所有的磁盘读；
- 如果读取在主机上成功，但在备机上失败（反之亦然）怎么办？必须做一些额外的工作来处理失败的磁盘读操作。
实验表明备机执行磁盘读取会降低1-4%吞吐，但同时也降低了日志带宽

### FAQ
GFS和VMware FT都提供了容错性，哪一个更好？
FT提供==计算==容错，可以为任何已有的网络服务器提供容错性
GFS提供==存储==容错，只针对存储服务提供容错性，备份策略相比FT更加高效

==VMware FT仅支持单处理器，多核并行所涉及的不确定性论文中并没有解决==

GFS 通过 **chunk 多副本复制** 保证**存储字节**一致；VMware FT 通过 **logging replication** 保证**CPU/内存状态**一致。二者复制对象不同，解决的是不同层面的“容错”。

## 05 Raft
参考：[微信公众平台](https://mp.weixin.qq.com/s?__biz=MzIwODA2NjIxOA==&mid=2247484140&idx=1&sn=37876b5dda5294ea7f6211f0a3300ea5&chksm=97098129a07e083fe65f8b87c2ec516b630a8f210961038f0091fbcd69468b41edbe193891ee&scene=21#wechat_redirect)
Raft（分布式共识算法）的目标是：==保证log完全相同的复制到多台服务器==
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/6548f2ab8944c44f68099fed34c9d368_MD5.jpeg)
### 系统模型
假设：
- 服务器可能宕机，但是非拜占庭的
- 网络通信会中断；可能会网络分区
Raft基于Leader的共识算法，需考虑“
- Leader正常运行
- Leader故障， 需要重新选举

==Leader改变时，系统可能会处于不一致的状态。因此，下一任Leader需要进行清理；==


## 06 ZooKeeper设计原理
Zookeeper是一个分布式协调服务，提供：
- 统一命名服务
- 配置管理
- 成员管理
- Leader 选举
- 协调分布式事务
- 分布式锁
Zookeeper是一个独立、通用的服务，用来提供API帮助开发者构建分布式应用。相较于Raft来说，Raft并不是一个可以直接交互的独立服务。Zookeeper建立在Zab之上。

> ZAB：Zookeeper专用协议，强调事务顺序一致性，适合读多写少的协调类系统（分布式锁、配置中心）
> Raft：通用性强，适合频繁读写的分布式系统（ETCD、Consul）
### Zookeeper技术架构
与Raft类似，Zookeeper分为Leader和Follower；但不同的是，Raft只有Leader处理读写请求，==Zookeeper允许所有节点处理读请求，写操作仍发给Leader==，虽然会读到过时的数据，但是提高了读性能。
Raft中，当加入更多的服务器，Leader会成为瓶颈。
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/589bf60284764df95c8c20e8859598d9_MD5.jpeg)
==Zookeeper专门为大量读负载而设计的系统==
> 线形一致性：要求系统在全局范围内保持操作的顺序一致性

==Zookeeper有两个基本的一致性保证：线形写、FIFO的客户端请求==

### 线形写
Leader保证写操作的顺序，并且顺序在所有Follower保持一致。使用Serializable而不是线性一致性（linearizability）
==Zookeeper只保证线形写，不保证线形读==

### FIFO的客户端请求
每个客户端可以为其操作（读和写）制定一个顺序，Zookeeper按照客户端指定的顺序来执行。
对于写请求，保证线形写，但是不保证严格的顺序关系
对于读请求，通过zxid实现，zxid是最后一个事务的标记，客户端请求会带上zxid，副本检查保证读到的是最新的数据
==只能说Zookeeper对于单个客户端请求是线性一致的==

### 同步操作sync
Zookeeper提供sync操作，本质是一个写请求，通过sync可以使最终所有副本中都是最新的数据
sync本质就是一个写请求，然后后面跟了一个读请求，保证读请求能够读到自己写请求的内容
==sync代价很高==

### 数据模型
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/1f841e5461bdb53bff470bfe2fe4ca68_MD5.jpeg)
两种类型的znode：
- 普通（Regular），持久的：持久化存储的普通节点
- 临时：会话结束自动删除的节点
所有znode都存储数据。临时znode没有子节点
创建新的znode节点时，还可以指定seqno创建顺序的znode
总共四种类型的znode：
- `PERSISTENT`
- `EPHEMERAL`
- `PERSISTENT_SEQUENTIAL`
- `EPHEMERAL_SEQUENTIAL`

### Zookeeper实现
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/4feacc805c9fe6572c7d58084445c7cf_MD5.jpeg)
- **请求处理器(Request Processor)**：将收到的请求转为**幂等的**事务，根据版本信息，生成包含新数据、版本号和更新的时间戳的 `setDataTXN`
- **原子广播（Atomic Broadcast）**：使用 zab 协议达成共识，这里不展开
- **多副本数据库（Replicated Database）**：将内存状态存储为模糊快照(fuzzy snapshot)，用来恢复状态。做快照时不锁定当前状态。
从fuzzy snapshot中恢复的状态可能不是Zookeeper数据的最终状态，需要Zab重新发送状态变更，最终保证与宕机前的服务状态一致。

### 客户端API
几个重要特性：
- 排他性的create，有且仅有一个create返回成功
- 用watch来避免轮询，单次触发，异步通知

### Zookeeper应用
- VMware-FT的`test-and-set`服务
- GFS中的master可以使用Zookeeper来扮演，所有副本都可以提供读服务，不仅提高了性能，并且实现HA
- MapReduce中管理成员信息

### FAQ
Q：线性化和串行化有什么不同？
**线性化**关注单个对象上的单个操作，要求这些操作按照真实时间顺序原子的执行
**串行化**针对包含多个操作的事务，要求事务的执行结果等效于某个串行执行顺序，不需要严格的实时顺序

Q：什么是流水线
**ZooKeeper 的流水线 = 客户端异步并发发送请求，减少往返延迟；与 Leader 端的 batching（合并刷盘）互补，共同提升整体吞吐。**

Q：什么是wait-free
一个实现被称为 wait-free，当且仅当  
**“任何进程都能在有限步骤内完成自己的操作，而无需依赖其他进程的速度或是否发生故障。”**
ZooKeeper 服务端使用单线程模型来处理客户端的请求，所以是 wait-free 的，因为它在处理一个客户端请求的时候，无需等待其他客户端的结果。
不过，根据 watch 机制，ZooKeeper 客户端有时候也会等待别的客户端的结果。

Q：为什么要实现fuzzy snapshots？
精确的快照需要==防止在创建快照和写入快照时发生任何写操作==，降低性能。
ZooKeeper 的快照是将内存状态导出写入持久存储:
- 异步线程生成快照文件；
- 快照文件名的后缀是最后提交的事务的 `zxid`；
- 快照文件生成过程中，仍然有新的事务提交；
- 因此，**快照文件不是精确到某一时刻的快照文件，可能与实际存在的任何数据树都不对应，因此是模糊的**；
- 这就要求事务操作是幂等的，否则产生不一致；

Q：什么是universal object？
Universal object 的核心思想是：某些并发数据结构或操作足够强大，能够用来实现任何其他并发数据结构或操作。
在实际应用中，ZooKeeper 的通用性体现在以下几个方面：
- **分布式锁**：通过 `create` 操作创建临时顺序节点，可以实现分布式锁。
- **选举**：通过 `create` 操作创建临时顺序节点，并选择最小的节点作为领导者，可以实现领导者选举。
- **队列**：通过 `create` 操作创建临时顺序节点，可以实现分布式队列。
- **配置管理**：通过 `setData` 和 `getData` 操作，可以实现配置的动态更新和通知。

### 小结
Zookeeper是一个为特定用途设计，放宽了一致性，以提高以读为主的工作负载，Zookeeper的吞吐量可以线形拓展

## 10 Patterns and Hints for Concurrency in GO
能简化程序理解的前提下，同一个代码的实现，可以考虑将==数据状态==转换为==代码状态==

