## 01 Introduction
### 分布式底层基础架构：
- Storage，存储基础架构，比如键值服务器、文件系统等
- Computation，计算框架，用来编排或构件分布式应用程序，比如经典的mapreduce
- Comminication，分布式系统逃不开网络通信问题，比如后续会讨论的RPC(远程过程调用)
RPC提供的语义：
- **at most once：最多一次**
- **exactly once：恰好一次**
- **at least once：至少一次**

分布式系统的3个重要特性：
**fault tolerance 容错性：**
包括可用性，用p999指标度量，依赖replication
可恢复性，依赖logging 或者 transaction 以及 durable storage

**consistency 一致性**：
强一致性、最终一致性

**performance 性能**：
throughout 吞吐量
latency 低延迟，避免尾部延迟（一台机器执行慢，拖慢整个请求响应）

==需要做到一致性与性能的权衡==

### MapReduce
具体参考：[mp.weixin.qq.com/s/I0PBo\_O8sl18O5cgMvQPYA](https://mp.weixin.qq.com/s/I0PBo_O8sl18O5cgMvQPYA)
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/d4fa0b3b50f4a3ecd6d958ed899ba97a_MD5.jpeg)
**工作流：**
1. 将输入文件分成M个小文件，集群启动MapReduce实例，包括一个Master和多个Worker
2. Master分配任务，将Map任务分配给Worker节点
3. Map Worker读取文件，执行用户定义的map函数，输出key、value，缓存在内存中
4. 内存中的key、value通过partitioning function分为R个regions，写入磁盘，将文件地址回传给Master，Master将地址传给Reduce Worker
5. Reduce Worker收到文件位置信息，通过RPC获取数据，根据key排序，将同一key的数据分组聚合
6. Reduce Worker将分组后的值执行用户自定义的reduce函数，输出追加到所属分区的输出文件中
7. 当所有Map、Reduce任务完成后，Master向用户程序返回结果
**实例：**
词频统计

**容错性**：
Worker故障：Master向Worker发送心跳包，指定时间内没返回，标记为Worker失效，将任务分配给其他Worker
Master故障：终止子整个MapReduce任务，重新执行

**性能**：
**网络带宽匮乏**：尽量将Map任务分配给包含输入数据的机器上执行，如果找不到，就近分配
**“落伍者（Stragglers）”**：单点机器执行慢，通过备用任务（backup tasks）处理，Master调度其他进程同时处理该任务，其中一个执行完成， 就标记为任务完成。

**Combiner函数**：
在某些情况下，Map函数产生的中间key值重复数据多，用户可以自定义一个Combiner函数，先将本地的记录进行一次合并，然后再将合并的结果通过网络发送出去。
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/d80f085916fe694320c83ab33b53f2b0_MD5.jpeg)
==Combiner函数通常和Reduce函数的代码相同，使用Combiner的好处是减少网络的通信量==
为什么默认禁用Combiner？
Combiner与Reducer一个是本地聚合，一个是全局聚合，逻辑会有偏差，会影响到执行结果的正确性。

## 02 RPC and Threads
参考：[微信公众平台](https://mp.weixin.qq.com/s?__biz=MzIwODA2NjIxOA==&mid=2247484193&idx=1&sn=693e0ff4bfcc6e02dea10ed9d639b41b&chksm=970980e4a07e09f2647de63ed0bf3be98d9032a3797033af3872c692d2373f98627a63f30e22&cur_album_id=1751707148520112128&scene=189#wechat_redirect)
Go通过channel、sync.Cond、WaitGroup解决线程问题。
GO RPC实现了==At Most Once==语义，如果没有得到响应，只会返回一个错误。客户端可以重试一个失败请求，服务端需要处理重复请求的情况：
- 如何保证多个客户端的 ID 是唯一的？可以带上客户端 ID，类似于：`<client_id, seq>`(和 Raft 客户端交互那部分内容对应上了！)
- 但我们不可能无期限地保存所有的请求 ID，保存多长时间？可以在客户端的请求中包含一个额外的标识符 X，告诉服务端删除 X 之前的所有请求 ID 是安全的
- 当原始请求还在执行时，如何处理重复的请求？可以等待它完成，也可以直接忽略新的请求。
- 为了避免服务器宕机，ID 信息还需要写入到磁盘，也许还要跨机器多副本存储。

## 03 GFS
参考：[mp.weixin.qq.com/s?\_\_biz=MzIwODA2NjIxOA==&mid=2247484205&idx=1&sn=ff45aeaeac29b79a6fad53f912a16933&chksm=970980e8a07e09feebe17c4cde354cd3caf1432ea0897ba4db7d08f05ffe245ec46773e36fb9&cur\_album\_id=1751707148520112128&scene=189#wechat\_redirect](https://mp.weixin.qq.com/s?__biz=MzIwODA2NjIxOA==&mid=2247484205&idx=1&sn=ff45aeaeac29b79a6fad53f912a16933&chksm=970980e8a07e09feebe17c4cde354cd3caf1432ea0897ba4db7d08f05ffe245ec46773e36fb9&cur_album_id=1751707148520112128&scene=189#wechat_redirect)

### GFS的目标
- 大型：大容量，需要存放大量的数据集；
- 性能：自动分片(Auto-Sharding)；
- 全局：不只是为一个应用而定制，适用于各种不同的应用；
- 容错：自动容错，不希望每次服务器出了故障，都要手动去修复；
### 架构
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/61e85453876b90e37c380a2d7298b277_MD5.jpeg)
GFS集群包含一个master和多个chunkserver，并且有多个client进行交互。
**chunk**：存储在GFS中的文件会被分为多个chunk，每个chunk大小为64M，master会分配一个不可变、全局唯一的64位ID（chunk handle）；默认一个chunk有三个副本，在不同的chunk server上。
**master**：维护文件系统的metadata；负责chunk的迁移、reblancing和垃圾回收；通过心跳与chunkserver通信
**client**：向master询问文件metadata，去对应的chunkserver获取数据
**chunkserver**：存储chunk，chunkserver月client不会缓存chunk数据，防止数据的不一致

### master节点
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/b50ae88811249a943c7411edcf0a6181_MD5.jpeg)
GFS只有一个master；master存储3种metadata，标记nv（non-volatile）需要在写入的同时存到磁盘，标记为v的数据master启动后查询chunkserver集群：
- namespace和文件名 nv
- 文件名-》chunk的映射 nv
- chunk-〉版本号（nv）、chunkservers（v）、primary（v）、租约（v）
master会在本地磁盘存储log，而不是存在数据库；相比于数据库操作，追加log很高效，并且通过在log中插入checkpoint点，重建状态也会很快

### 读文件
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/f2a393db68896a61123940e556135be2_MD5.jpeg)
- client 将 文件名+offset 转为文件名+ `chunk index`，向 master 发起请求；
- master 在内存中的 metadata 查询对应 chunk 所在的 `chunk handle` + `chunk locations` 并返回给 client；
- client 将 master 返回给它的信息缓存起来，用文件名 + `chunk index` 作为 key；(**注意：client 只缓存 metadata，不缓存 chunk 数据**)
- client 会选择网络上最近的 chunkserver 通信(Google 的数据中心中，IP 地址是连续的，所以可以从 IP 地址差异判断网络位置的远近)，并通过 `chunk handle` + `chunk locations` 来读取数据；
### 租约
如果每次写都要请求master，那么master会成为性能瓶颈；master首先会找到拥有该chunk的chunkserver，为其中一个chunkserver授予==租约==，拥有租约的chunkserver称为==Primary==，其他称为==Secondary==
- master 会增加版本号，并将版本号写入磁盘，然后 master 会向 `Primary` 和`Secondary` 副本对应的服务器发送消息并告诉它们，谁是 `Primary`，谁是 `Secondary`，最新的版本号是什么；
- 在租约有效期内，对该 chunk 的写操作都由 `Primary` 负责；
- 租约的有效期一般为 60 秒，租约到期后 master 可以自由地授予租约；
- master 可能会在租约到期前撤销租约(例如：重命名文件时)；
- 在写 chunk 时，`Primary` 也可以请求延长租约有效期，直至整个写完 chunk；
### 写文件
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/ff50f1b2b1346b8de435f1288bc43583_MD5.jpeg)
1. client 向 master 询问 `Primary` 和 `Secondary`。如果没有 chunkserver 持有租约，master 选择一个授予租约；
2. master 返回 `Primary` 和 `Secondary` 的信息，client 缓存这些信息，只有当 `Primary` 不可达或者**租约过期**才再次联系 master；
3. client 将追加的记录发送到**每一个 chunkserver(不仅仅是 `Primary`)**，chunkserver 先将数据写到 LRU 缓存中(不是硬盘！)；
4. 一旦 client 确认每个 chunkserver 都收到数据，client 向 `Primary` 发送写请求，`Primary` 可能会收到多个连续的写请求，会先将这些操作的顺序写入本地；
5. `Primary` 做完写请求后，将写请求和顺序转发给所有的 `Secondary`，让他们以同样的顺序写数据；
6. `Secondary` 完成后应答 `Primary`；
7. `Primary` 应答 client 成功或失败。如果出现失败，client 会重试，但在重试整个写之前，会先重复步骤 3-7；
### 一致性模型
GFS是一个弱一致性模型，并不保证一个chunk的所有副本是相同的。如果一个写失败，client会重试
- 对于写：可能有部分副本成功，而另一部分失败，副本就会不一致。
- 对于 `record append`：也会重试，但是不是在原来的 offset 上重试，而是在失败的记录后面重试，这样 `record append` 留下的不一致是永久的不一致，并且会让副本包含重复的数据。

### 快照
GFS通过snapshot创建一个文件或者目录树的备份，勇于备份文件或者创建checkpoint。GFS使用写时复制COW来写快照

### 数据完整性
每一个chunkserver使用checksum来检验存储数据的完整性
每个chunk以64kb的块进行划分，每一个块对应一个32位的checksum

### 总结
GFS最严重的局限性是只有一个master节点
- 随着 GFS 的应用越来越多，文件也越来越多，最后 master 会耗尽内存来存储 metadata；你可以增加内存，但单台计算机的内存始终有上限；
- master 节点要承载数千个 client 的请求，master 节点的 CPU 每秒只能处理数百个请求，尤其是还要将部分数据写入磁盘——client 的数量会超过单个 master 的能力；
- 弱一致性会导致应用程序很难处理 GFS 奇怪的语义；
- 最后一个问题，master 的故障切换不是自动的，需要人工干预来处理已经永久故障的 master 节点，并更换新的服务器，这需要几十分钟甚至更长的时间来处理。对于某些应用程序来说，这个时间太长了。

## 04 主从复制（Primary/Backup Replication）
这一课讨论关于容错(Fault-Tolerance)和复制(Replication)的问题，主要研究 VMware FT 的论文 —— **The Design of a Practical System for Fault-Tolerant Virtual Machines**

### 复制的方式
- 状态转移（State Transfer）：Primary将自己所有的状态拷贝发送给Backup备机，一般是增量备份
- 复制状态机（Replicated State Machine）：client发送操作给主机，主机按照顺序发送到备机

VMware FT使用了复制状态机的方法。
> 状态转移是内存，复制状态机是客户端的操作。倾向于使用复制状态机的原因是，外部操作比服务的内存状态要小。

### 复制的挑战
- 我们要复制哪些状态？
- 主机必须等待备机备份完吗？
- 什么时候切换到备机？
- 切换时能否看到异常情况？
- 如果有个副本故障了，我们需要重新添加一个新的副本，这可能是一个代价很高的行为，因为副本可能非常大，如何提升添加新副本的速度？

### VMware FT论文总结
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/fa48e631d48d5121c88197949d17709c_MD5.jpeg)
Primary VM为主机，Backup VM为备机，虚拟机的虚拟磁盘在共享存储上。
所有的输入输入到主机，然后通过Logging channel转发到备机。
==两台虚拟机都会执行输入，但是只有主机的输出会返回给客户端，备机的输出被管理程序丢弃==

### 确定性重放（Deterministic replay）
不确定性事件比如虚拟中断，会让主机与备机的运行结果不一致。
VMware通过==确定性重放==捕获所有的输入和可能的不确定输入，写入到日志文件记录下来。

### FT协议
VMware通过确定性重放产生日志，但不将日志写入磁盘，而是用过Logging channel发送给备机，备机实时重放日志项。
为了容错，需要在Logging channel上实现容错协议，要求：
**输出要求**：如果备机在主机故障后接管，备机将以和主机一致的方式运行
**输出规则**：主机直到备机接受并确定了和输出相关的日志，才发送输出给外界
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/09f0792b520d4ad7dd323f2e0cfe8188_MD5.jpeg)
==每个复制系统都有这个问题：在某个时间点，主机必须停下来等待备机，这会影响性能==

### 发现与处理故障
通过==udp心跳包==和==监控Logging channel上的流量==相结合来检测，如果心跳包超时或logging channel流量停止则表明故障
如果备机故障，主机停止向logging channel发送日志

> 备机故障后如何追上主机？
> VMware通过VMotion，能够在最小程度上终端虚拟机的运行， 克隆一个虚拟机

如果主机故障，备机必须重放完所有日志，然后备机接管主机
为了确保一次只有一个虚拟机称为主机，避免脑裂，Vmware在共享存储上会执行一个原子`test-and-set`锁指令，只有一个机器返回成功

### FT的实现细节
VMware VMotion最小化中断，将运行中的虚拟机从一台服务器迁移到另一台服务器，这种克隆操作打断主机的时间不超过1秒
**管理Logging Channel**
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/789f74b24fc2890acab629367c4882e9_MD5.jpeg)
通常，主机日志缓冲充满的原因：
- 带宽太小，**建议日志通道带宽 1Gbit/s**；
- 备机执行速度太慢，从而消费日志太慢时，主机的日志缓冲也可能会被填满；
VMware FT中实现了一种机制，当备机远远落后时（落后1s），可以减缓主机的执行速度。通常只会在系统处于极端压力的情况下发生。

### 替代方案
![](%E5%88%86%E5%B8%83%E5%BC%8F/attachments/d77de288a6f9ba3d8f766eac82421918_MD5.jpeg)
==共享磁盘和非共享磁盘==：VMware FT中使用了一个主备机都能访问的共享存储。替代方案是使用单独的虚拟磁盘，优点是：存储不能同时被主备机访问，共享存储太贵；缺点：需要同步磁盘状态
==备机上执行磁盘读取==：目前实现中，备机不会从磁盘中读取，因为磁盘操作被认为是一种输入。替代方案是备机可以进行磁盘读取，当大量磁盘读的工作负载，这种方式可以减少日志通道的流量，缺点：
- 可能会减慢备机的执行速度，因为备机必须执行所有的磁盘读；
- 如果读取在主机上成功，但在备机上失败（反之亦然）怎么办？必须做一些额外的工作来处理失败的磁盘读操作。
实验表明备机执行磁盘读取会降低1-4%吞吐，但同时也降低了日志带宽

### FAQ
GFS和VMware FT都提供了容错性，哪一个更好？
FT提供==计算==容错，可以为任何已有的网络服务器提供容错性
GFS提供==存储==容错，只针对存储服务提供容错性，备份策略相比FT更加高效

==VMware FT仅支持单处理器，多核并行所涉及的不确定性论文中并没有解决==

GFS 通过 **chunk 多副本复制** 保证**存储字节**一致；VMware FT 通过 **logging replication** 保证**CPU/内存状态**一致。二者复制对象不同，解决的是不同层面的“容错”。