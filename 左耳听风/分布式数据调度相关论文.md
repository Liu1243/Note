[29 \| 推荐阅读：分布式数据调度相关论文 - 极客时间已完结课程限时免费阅读](https://freegeektime.com/100002201/2421/)
分布式系统的一个关键技术是“数据调度”。因为我们需要扩充节点，提高系统的高可用性，所以必须冗余数据结点。建立数据结点的副本看上去容易，但其中最大的难点就是分布式一致性的问题。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/d4047c174bc4e6d19ea3a910c665bf00_MD5.jpeg)

## Paxos算法
Paxos 定义了三类角色（可以由同一节点同时承担）：
1. **Proposer（提议者）**  
    负责提出提案（proposal），希望系统接受某个值。
    
2. **Acceptor（接受者）**  
    负责投票（accept）。多数 Acceptor 的一致决定代表共识达成。
    
3. **Learner（学习者）**  
    最终知道哪个值被选定。

Paxos 算法流程（两阶段）
Paxos 核心是 **两阶段提交**，带有编号（proposal number）防止冲突：
阶段一：Prepare 阶段
1. Proposer 生成一个唯一且递增的提案号 `n`，发送给所有 Acceptor：“能否接受我编号 `n` 的提案？”
    
2. Acceptor 收到后：
    
    - 如果 `n` 大于它之前承诺过的编号，就承诺 **不再接受比 `n` 小的提案**；
        
    - 并返回它之前已接受过的最大编号的提案（如果有）。
        
阶段二：Accept 阶段
1. Proposer 收到多数 Acceptor 的响应后：
    
    - 如果有 Acceptor 曾经接受过某个值，它必须 **继承那个值**（保持一致性）；
        
    - 如果没有，就可以自由选择自己的值。
        
2. Proposer 发送 **“接受编号 n，值 v”** 给所有 Acceptor。
    
3. Acceptor 如果之前没承诺过更大的编号，就接受这个提案。
    
当多数 Acceptor 接受 `(n, v)` 时，值 `v` 就被选定（Chosen）。

为什么能够保持一致性？
- **多数派原则**：任何两个多数集合必有交集。即使后续提案被提出，交集中的 Acceptor 会强制它继承之前的值，保证不会分裂。
- **编号约束**：新的提案编号总是更大，从而不会被旧提案覆盖。

### Multi-Paxos
**Multi-Paxos** 就是把 Paxos 扩展成**连续多次达成共识**的机制。
Multi-Paxos 的核心思路：
- **第一轮**：运行一次完整 Paxos（Prepare + Accept），确定领导者（Leader）。
- **后续多轮**：只要领导者不失效，大家就**跳过 Prepare 阶段**，直接进入 Accept 阶段来提案新的值。

Multi-Paxos的角色演变：
- **Leader（领导者）**：相当于稳定的 Proposer，负责发起提案。
- **Followers（跟随者）**：充当 Acceptors，只要 Leader 有效，就跟随它的提案。
- **Learners**：从 Acceptors 得知结果，更新状态机。

Multi-Paxos 流程
阶段一：选出 Leader（Prepare）
- 某个节点（Proposer）发起 **Prepare(n)** 请求。
    
- 多数 Acceptor 承诺后，它成为 **Leader**。
    
阶段二：连续达成共识（Accept）
- Leader 直接向多数 Acceptor 发出 **Accept 请求**（附带提案号和值）。
    
- Acceptor 接受后，值就确定。
    
- 每次确定一个新值，可以认为是在日志里追加一个条目。

Multi-Paxos可以避免每次都做 Prepare。

## Raft
Raft把这个一致性的算法分解成了几个部分，一个是领导选举（Leader Selection），一个是日志复制（Log Replication），一个是安全性（Safety），还有一个是成员变化（Membership Changes）。

## 逻辑钟和向量钟
> 查表式的计算分布（GFS）：
> 数据位置是 Master 明确记录的，客户端必须查 Master 才知道存哪。
> 计算式的数据分布（Dynamo）：
> 数据的位置可以通过哈希函数计算出来，不需要中心节点存表。

Dynamo中的Handoff机制：在目标节点不可用时，把数据临时存到其他节点，提升可用性
Dynamo 引入 **Hinted Handoff** 来平衡 **可用性** 与 **一致性**：
- **写入时检测节点是否不可用**。
    - 如果目标副本节点挂了，系统不能把数据丢掉。
- **临时交给“替补节点”存储副本**。
    - 这个替补节点会把数据保存下来，并附带一个 “hint”（提示），记录本来应该属于哪个节点。
- **目标节点恢复时，替补节点会把数据交还给它**。
    - 一旦原节点重新上线，替补节点会把这段数据“交接回去”，恢复正常副本布局。

Dynamo文章中的关键概念就是Vector Clock，Gossip协议。
提到向量时钟就需要提一下逻辑时钟。所谓逻辑时间，也就是在分布系统中为了解决消息有序的问题，由于在不同的机器上有不同的本地时间，这些本地时间的同步很难搞，会导致消息乱序。
逻辑时钟可以保证，如果事件A先于事件B，那么事件A的时钟一定小于事件B的时钟，但是返过来则无法保证，因为返过来没有因果关系。所以，向量时钟解释了因果关系。向量时钟维护了数据更新的一组版本号（版本号其实就是使用逻辑时钟）。

> 逻辑时钟 vs 向量时钟
> 为什么需要时钟？
> 各节点的 **物理时钟不同步**（即使用 NTP，也可能有漂移）。但是很多场景需要知道事件的先后关系（判断两个写操作是否有因果关系；解决版本冲突）
> 引入 **逻辑时钟** 概念：不依赖物理时间，而是用“逻辑时间”来表示事件的偏序关系。

|特性|Lamport Clock（逻辑时钟）|Vector Clock（向量时钟）|
|---|---|---|
|存储开销|每个节点一个整数|每个节点一个 N 维向量|
|保证|因果关系 → 时间单调递增|能区分因果关系 vs 并发|
|能否判断并发|❌ 不能|✅ 能|
|应用场景|分布式互斥、事件排序|版本控制、冲突检测（Dynamo、Cassandra）|

## Gossip协议
DynamoDB中使用到了Gossip协议来做数据同步。Gossip算法也是Cassandra使用的数据复制协议。
节点之间存在三种通信方式：
- push方式。A节点将数据(key,value,version)及对应的版本号推送给B节点，B节点更新A中比自己新的数据。
- pull 方式。A仅将数据key,version推送给B，B将本地比A新的数据(key,value,version)推送给A，A更新本地。
- push/pull方式。与pull类似，只是多了一步，A再将本地比B新的数据推送给B，B更新本地。
如果把两个节点数据同步一次定义为一个周期，那么在一个周期内，push需通信1次，pull需2次，push/pull则需3次。从效果上来讲，push/pull最好，理论上一个周期内可以使两个节点完全一致。直观感觉上，也是push/pull的收敛速度最快。
另外，每个节点上又需要一个协调机制，也就是如何交换数据能达到最快的一致性——消除节点的不一致性。上面所讲的push、pull等是通信方式，协调是在通信方式下的数据交换机制。
协调所面临的最大问题是，一方面需要找到一个经济的方式，因为不可能每次都把一个节点上的数据发送给另一个节点；另一方面，还需要考虑到相关的容错方式，也就是当因为网络问题不可达的时候，怎么办？
一般来说，有两种机制：一种是以固定概率传播的Anti-Entropy机制，另一种是仅传播新到达数据的Rumor-Mongering机制。前者有完备的容错性，但是需要更多的网络和CPU资源，后者则反过来，不耗资源，但在容错性上难以保证。
Anti-Entropy的机制又分为Precise Reconciliation（精确协调）和Scuttlebutt Reconciliation（整体协调）这两种。前者希望在每次通信周期内都非常精确地消除双方的不一致性，具体表现就是互发对方需要更新的数据。因为每个结点都可以读写，所以这需要每个数据都要独立维护自己的版本号。
而整体协调与精确协调不同的是，整体协调不是为每个数据都维护单独的版本号，而是每个节点上的数据统一维护一个版本号，也就是一个一致的全局版本。这样与其他结果交换数据的时候，就只需要比较节点版本，而不是数据个体的版本，这样会比较经济一些。如果版本不一样，则需要做精确协调。

## 分布式数据库方面
### Aurora
Aurora 是 AWS 针对传统数据库（如 MySQL、PostgreSQL）的**云原生重构**版本，核心思想正是 **计算与存储分离**。

### Spanner
Spanner 是Google的全球分布式数据库（Globally-Distributed Database) 。

## 小结
分布式的服务的调度需要一个分布式的存储系统来支持服务的数据调度。而我们可以看到，各大公司都在分布式的数据库上做各种各样的创新，他们都在使用底层的分布式文件系统来做存储引擎，把存储和计算分离开来，然后使用分布式一致性的数据同步协议的算法来在上层提供高可用、高扩展的支持。
过去的分库分表并通过一个数据访问的代理服务的玩法，应该在不久就会过时就会成为历史。真正的现代化的分布式数据存储就是Aurora和Spanner这样的方式。
