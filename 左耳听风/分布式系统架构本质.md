## 分布式系统的冰与火
使用分布式系统的原因：
- 增大系统容量：多台机器应对大规模应用场景；垂直或水平拆分业务，变成一个分布式架构。

| 特性   | **垂直拆分**         | **水平拆分**        |
| ---- | ---------------- | --------------- |
| 拆分维度 | 按业务功能模块          | 按数据维度（用户/时间/哈希） |
| 解决问题 | **业务复杂度 / 模块解耦** | **单表/单库性能瓶颈**   |
| 典型场景 | 电商系统：用户、订单、支付    | 高并发大数据量：订单表分库分表 |
| 数据库  | 多个业务独立库          | 同业务多库分片         |
| 复杂性  | 服务间通信、事务一致性      | 跨分片查询、分布式事务     |
| 优点   | 易扩展、业务清晰、团队协作好   | 数据水平扩展，支持海量数据   |
| 缺点   | 依赖多、运维复杂         | 路由复杂、聚合查询难      |
- 加强系统可用：通过分布式架构来冗余系统以消除单点故障，从而提高系统的可用性。

单体应用和分布式架构的优缺点：
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/eb90768e579bb2971775f1c8c52d0eb2_MD5.jpeg)
分布式系统架构的难点在于系统设计，以及管理和运维。

### 分布式系统的发展
SOA-基于服务的架构
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/66922104f9bc640d32127429efbcacbd_MD5.jpeg)
- 20 世纪 90 年代前，是单体架构，软件模块高度耦合。
- 而 2000 年左右出现了比较松耦合的 SOA 架构，这个架构需要一个标准的协议或是中间件来联动其它相关联的服务（如 ESB）。其实是IoC和DIP设计思想在架构中的实践。
- 而 2010 年后，出现了微服务架构，这个架构更为松耦合。每个微服务自包含，服务的整合需要服务编排。

SOA和微服务的区别：

|对比点|SOA|微服务|
|---|---|---|
|核心思想|服务化|服务更小粒度化|
|通信方式|ESB 统一调度|轻量级 RPC/REST/消息队列|
|服务治理|中心化（依赖 ESB）|去中心化（服务发现+网关）|
|粒度|粗粒度服务|小而自治的服务|
|技术依赖|SOAP、WS-* 协议为主|REST/HTTP、gRPC 等|
|演进关系|微服务是 SOA 的进化版|来源于 SOA，但更轻量灵活|

编排和组织引擎可以是工作流引擎，也可以是网关。微服务在集成测试、运维和服务管理方面比较麻烦，需要一个比较好的微服务PaaS平台，就像Spring Cloud提供各种配置服务、服务发现等，还有K8S提供的部署和调度方式。

## 从亚马逊的实践谈分布式系统的难点
1. 分布式服务的架构需要分布式的团队架构。一个服务不超过16人（Two Pizza Team）负责，从前端到数据，从需求到上线运维，==按职责分工，而不是技能分工==
2. 分布式服务查错不容易。一旦出现比较严重的故障，需要整体查错。
3. 没有专职的测试人员，也没有专职的运维人员，开发人员做所有的事情。
4. 运维优先，崇尚简化和自动化。DevOps
5. 内部服务和外部服务一致。内部系统的服务随时都可以开放出来

### 分布式系统中需要注意的问题
问题1：异构系统的不标准问题
这主要表现在：
- 软件和应用不标准。
- 通讯协议不标准。
- 数据格式不标准。
- 开发和运维的过程和方法不标准。

好的配置管理：应该分成三层：底层和操作系统相关，中间层和中间件相关，最上面和业务应用相关。底层和中间层是不能让用户灵活修改的，而是只让用户选择。

问题2：系统架构中的服务依赖性问题
分布式架构下，服务是会有依赖的，一个服务依赖链上的某个服务挂掉了，可能会导致出现“多米诺骨牌”效应。
- 如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务。
- 服务依赖链中，出现“木桶短板效应”——整个 SLA 由最差的那个服务所决定。
这就是服务治理的内容，需要我们定义出服务的关键程度，还需要定义关键业务或服务调用的主要路径。

这里需要注意的是，很多分布式架构在应用层上做到了业务隔离，然而，在数据库结点上并没有。数据库方面也需要做出相应的隔离。我们不但要拆分服务，还要为每个服务拆分相应的数据库。

问题3：故障发生的概率更大
分布式系统虽然故障的影响面可以被隔离，但是由于机器和服务多，出故障的频率也会多。
- 出现故障不可怕，故障恢复时间过长才可怕。
- 出现故障不可怕，故障影响面过大才可怕。

问题4：多层架构的运维复杂度更大
我们可以把系统分成四层：基础层、平台层、应用层和接入层。
- 基础层就是我们的机器、网络和存储设备等。
- 平台层就是我们的中间件层，Tomcat、MySQL、Redis、Kafka 之类的软件。
- 应用层就是我们的业务软件，比如，各种功能的服务。
- 接入层就是接入用户请求的网关、负载均衡或是 CDN、DNS 这样的东西。
对于这四层，我们需要知道：
- 任何一层的问题都会导致整体的问题；
- 没有统一的视图和管理，导致运维被割裂开来，造成更大的复杂度。
很多公司按照技能分工，他们按照技能把技术团队分为产品开发、中间件开发、业务运维、系统运维等子团队。导致没有统一的运维视图，不知道一个服务调用是如何经过每一个服务和资源，也就导致在出现故障时要花大量的时间在沟通和定位问题上。

==分工不是问题，问题是分工后的协作是否统一和规范。==

## 分布式系统的技术栈
构建分布式系统的目的是增加系统容量，提高系统的可用性。转换为技术方面，就是完成：
- 大流量处理。通过集群技术把大规模并发请求的负载分散到不同的机器上。
- 关键业务保护。提高后台服务的可用性，把故障隔离起来阻止多米诺骨牌效应（雪崩效应）。如果流量过大，需要对业务降级，以保护关键业务流转。
一是提高整体架构的吞吐量，服务更多的并发和流量，二是为了提高系统的稳定性，让系统的可用性更高。

提高架构的性能
常用技术：
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/255b606466f27cb76fdb774da0f5b53f_MD5.jpeg)
- 缓存系统。加入缓存系统，可以有效地提高系统的访问能力。从前端的浏览器，到网络，再到后端的服务，底层的数据库、文件系统、硬盘和 CPU，全都有缓存，这是提高快速访问能力最有效的手段。对于分布式系统下的缓存系统，需要的是一个缓存集群。这其中需要一个 Proxy 来做缓存的分片和路由。
- 负载均衡系统。负载均衡系统是水平扩展的关键技术，它可以使用多台机器来共同分担一部分流量请求。
- 异步调用。异步系统主要通过消息队列来对请求做排队处理，这样可以把前端的请求的峰值给“削平”了，而后端通过自己能够处理的速度来处理请求。这样可以增加系统的吞吐量，但是实时性就差很多了。同时，还会引入消息丢失的问题，所以要对消息做持久化，这会造成“有状态”的结点，从而增加了服务调度的难度。
- 数据分区和数据镜像。数据分区是把数据按一定的方式分成多个区（比如通过地理位置），不同的数据区来分担不同区的流量。这需要一个数据路由的中间件，会导致跨库的 Join 和跨库的事务非常复杂。而数据镜像是把一个数据库镜像成多份一样的数据，这样就不需要数据路由的中间件了。你可以在任意结点上进行读写，内部会自行同步数据。然而，数据镜像中最大的问题就是数据的一致性问题。
初期会使用读写分离的数据镜像模式，后期采用分库分表的方式。

提高架构的稳定性
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/cd0eb32c953e2b7a616cd78b5396e7c8_MD5.jpeg)
- 服务拆分。服务拆分主要有两个目的：一是为了隔离故障，二是为了重用服务模块。但服务拆分完之后，会引入服务调用间的依赖问题。
- 服务冗余。服务冗余是为了去除单点故障，并可以支持服务的弹性伸缩，以及故障迁移。然而，对于一些有状态的服务来说，冗余这些有状态的服务带来了更高的复杂性。其中一个是弹性伸缩时，需要考虑数据的复制或是重新分片，迁移的时候还要迁移数据到其它机器上。
- 限流降级。当系统实在扛不住压力时，只能通过限流或者功能降级的方式来停掉一部分服务，或是拒绝一部分用户，以确保整个架构不会挂掉。这些技术属于保护措施。
- 高可用架构。通常来说高可用架构是从冗余架构的角度来保障可用性。比如，多租户隔离，灾备多活，或是数据可以在其中复制保持一致性的集群。总之，就是为了不出单点故障。
- 高可用运维。高可用运维指的是 DevOps 中的 CI/CD（持续集成 / 持续部署）。一个良好的运维应该是一条很流畅的软件发布管线，其中做了足够的自动化测试，还可以做相应的灰度发布，以及对线上系统的自动化控制。这样，可以做到“计划内”或是“非计划内”的宕机事件的时长最短。

分布式系统的关键技术
服务治理。服务拆分、服务调用、服务发现、服务依赖、服务的关键度定义……服务治理的最大意义是需要把服务间的依赖关系、服务调用链，以及关键的服务给梳理出来，并对这些服务进行性能和可用性方面的管理。
架构软件管理。服务之间有依赖，而且有兼容性问题，所以，整体服务所形成的架构需要有架构版本管理、整体架构的生命周期管理，以及对服务的编排、聚合、事务处理等服务调度功能。
DevOps。分布式系统可以更为快速地更新服务，但是对于服务的测试和部署都会是挑战。所以，还需要 DevOps 的全流程，其中包括环境构建、持续集成、持续部署等。
自动化运维。有了 DevOps 后，我们就可以对服务进行自动伸缩、故障迁移、配置管理、状态管理等一系列的自动化运维技术了。
资源调度管理。应用层的自动化运维需要基础层的调度支持，也就是云计算 IaaS 层的计算、存储、网络等资源调度、隔离和管理。
整体架构监控。如果没有一个好的监控系统，那么自动化运维和资源调度管理只可能成为一个泡影，因为监控系统是你的眼睛。没有眼睛，没有数据，就无法进行高效运维。所以说，监控是非常重要的部分。这里的监控需要对三层系统（应用层、中间件层、基础层）进行监控。
流量控制。最后是我们的流量控制，负载均衡、服务路由、熔断、降级、限流等和流量相关的调度都会在这里，包括灰度发布之类的功能也在这里。

Docker和K8S解决了上述大部分问题。

分布式系统的“纲”
分布式系统有5个关键句技术：
- 全栈系统监控；
- 服务 / 资源调度；
- 流量调度；
- 状态 / 数据调度；
- 开发和运维的自动化。
开发和运维的自动化，是需要把前四项都做到了，才有可能实现的。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/2a54f237bbfc3d398b31d55c6d210d2d_MD5.jpeg)

## 分布式系统关键技术：全栈监控
这个监控系统需要完成的功能为：
- 全栈监控；
- 关联分析；
- 跨系统调用的串联；
- 实时报警和自动处置；
- 系统性能分析。

### 多层体系的监控
全栈监控就是三层监控：
基础层：监控主机和底层资源。比如：CPU、内存、网络吞吐、硬盘 I/O、硬盘使用等。
中间层：就是中间件层的监控。比如：Nginx、Redis、ActiveMQ、Kafka、MySQL、Tomcat 等。
应用层：监控应用层的使用。比如：HTTP 访问的吞吐量、响应时间、返回码、调用链路分析、性能瓶颈，还包括用户端的监控。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/e72d98c1e6180a9432f37654b3e3b7a4_MD5.jpeg)
需要监控的标准化：日志数据结构化、监控数据格式标准化、统一的监控平台、统一的日志分析。

### 什么才是好的监控系统
不好的监控系统：
1. 监控数据是隔离开的
2. 监控的数据项太多
一个好的监控系统有以下特征：
关注于整体应用的 SLA。主要从为用户服务的 API 来监控整个系统。
关联指标聚合。把有关联的系统及其指标聚合展示。主要是三层系统数据：基础层、平台中间件层和应用层。其中，最重要的是把服务和相关的中间件以及主机关联在一起，服务有可能运行在 Docker 中，也有可能运行在微服务平台上的多个 JVM 中，也有可能运行在 Tomcat 中。总之，无论运行在哪里，我们都需要把服务的具体实例和主机关联在一起，否则，对于一个分布式系统来说，定位问题犹如大海捞针。
快速故障定位。对于现有的系统来说，故障总是会发生的，而且还会频繁发生。故障发生不可怕，可怕的是故障的恢复时间过长。所以，快速地定位故障就相当关键。快速定位问题需要对整个分布式系统做一个用户请求跟踪的 trace 监控，我们需要监控到所有的请求在分布式系统中的调用链，这个事最好是做成没有侵入性的。
一个好的监控系统主要为以下两个场景设计：
“体检”：
容量管理。提供一个全局的系统运行时数据的展示，可以让工程师团队知道是否需要增加机器或者其它资源。
性能管理。可以通过查看大盘，找到系统瓶颈，并有针对性地优化系统和相应代码。
“急诊”：
定位问题。可以快速地暴露并找到问题的发生点，帮助技术人员诊断问题。
性能分析。当出现非预期的流量提升时，可以快速地找到系统的瓶颈，并帮助开发人员深入代码。

如何做出一个好的监控系统
- 服务调用链跟踪。这个事情的最佳实践是 Google Dapper 系统，其对应于开源的实现是 Zipkin。对于 Java 类的服务，我们可以使用字节码技术进行字节码注入，做到代码无侵入式。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/46560a942b15d4cb7dfbaf7057d8f51f_MD5.jpeg)
- 服务调用时长分布。使用 Zipkin，可以看到一个服务调用链上的时间分布，这样有助于我们知道最耗时的服务是什么。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/a9bcf5686925ba71735a8ede7aa088ef_MD5.jpeg)
- 服务的TOP N视图。a）按调用量排名，b) 按请求最耗时排名，c）按热点排名（一个时间段内的请求次数的响应时间和）
- 数据库操作关联。对于Java，可以使用JavaAgent字节码注入技术拿到JDBC操作数据库的执行时间，与相关请求对应起来。
- 服务资源跟踪。我们需要把服务运行的机器节点上的数据（如 CPU、MEM、I/O、DISK、NETWORK）关联起来。

有了以上数据上的关联，可以达到以下的目标：
1. 当一台机器挂掉是因为 CPU 或 I/O 过高的时候，我们马上可以知道其会影响到哪些对外服务的 API。
2. 当一个服务响应过慢的时候，我们马上能关联出来是否在做 Java GC，或是其所在的计算结点上是否有资源不足的情况，或是依赖的服务是否出现了问题。
3. 当发现一个 SQL 操作过慢的时候，我们能马上知道其会影响哪个对外服务的 API。
4. 当发现一个消息队列拥塞的时候，我们能马上知道其会影响哪些对外服务的 API。
一旦了解了这些信息，就可以做出调度：
- 发现某个服务过慢是因为CPU使用过多，做弹性伸缩。
- 某个服务过慢是因为MySQL出现了慢查询，无法在应用层上做弹性伸缩，只能做流量限制，或者降级操作。
一个分布式系统，或是一个自动化运维系统，或是一个 Cloud Native 的云化系统，最重要的事就是把监控系统做好。在把数据收集好的同时，更重要的是把数据关联好。这样，我们才可能很快地定位故障，进而才能进行自动化调度。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/3d3527fe83b3966ae91e0fd5aa47692e_MD5.jpeg)

## 分布式系统关键技术：服务调度
### 服务关键程度和服务的依赖关系
服务关键程度要细致地管理对业务的理解，才能定义出架构中各个服务的重要程度。
因为依赖关系就像“铁锁连环”一样，一个服务的问题很容易出现一条链上的问题。传统的 SOA 希望通过 ESB 来解决服务间的依赖关系，这也是为什么微服务中希望服务间是没有依赖的，而让上层或是前端业务来整合这些后台服务。
是不可能做到真正的服务无依赖，总有一些公共服务会被依赖，只能降低服务依赖的深度和广度。
**微服务是服务依赖最优解的上限，而服务依赖的下限是千万不要有依赖环。**
解决服务依赖环的方案一般是，依赖倒置的设计模式。
服务的依赖关系可以通过Zipkin服务调用跟踪系统发现。

### 服务状态和生命周期的管理
我们需要有一个服务注册中心，来知道：
整个架构中有多少种服务？
这些服务的版本是什么样的？
每个服务的实例数有多少个，它们的状态是什么样的?
每个服务的状态是什么样的？

有了服务的状态和运行情况，需要对服务的生命周期进行管理。服务的生命周期会有以下状态：
Provision，代表在供应一个新的服务；
Ready，表示启动成功了；
Run，表示通过了服务健康检查；
Update，表示在升级中；
Rollback，表示在回滚中；
Scale，表示正在伸缩中（可以有 Scale-in 和 Scale-out 两种）；
Destroy，表示在销毁中；
Failed，表示失败状态。

有了服务的状态和生命周期的管理，以及服务的重要程度和服务的依赖关系，可以再加上一个服务运行状态的拟合控制，就可以管理整个分布式服务。

### 整个架构的版本管理
亚马逊有VersionSet管理整个架构的版本。
版本控制主要考虑各个服务版本的兼容性问题。
需要一个架构的 manifest，一个服务清单，这个服务清单定义了所有服务的版本运行环境，其中包括但不限于：
- 服务的软件版本；
- 服务的运行环境——环境变量、CPU、内存、可以运行的节点、文件系统等；
- 服务运行的最大最小实例数。

### 资源/服务调度
服务和资源调度的过程，与操作系统调度进程的方式很相似，主要有以下一些关键技术：
服务状态的维持和拟合。
服务的弹性伸缩和故障迁移。
作业和应用调度。
作业工作流编排。
服务编排。

### 服务状态的维持和拟合
服务状态是服务的运行状态，也就是Provision、Ready等。
服务运行过程中，状态也是会有变化的，这样的变化有两种：
一种是没有预期的变化。比如，服务运行因为故障导致一些服务挂掉，或是别的什么原因出现了服务不健康的状态。而一个好的集群管理控制器应该能够强行维护服务的状态。在健康的实例数变少时，控制器会把不健康的服务给摘除，而又启动几个新的，强行维护健康的服务实例数。
另一种是预期的变化。比如，我们需要发布新版本，需要伸缩，需要回滚。这时，集群管理控制器就应该把集群从现有状态迁移到另一个新的状态。这个过程并不是一蹴而就的，集群控制器需要一步一步地向集群发送若干控制命令。这个过程叫“拟合”——从一个状态拟合到另一个状态，而且要穷尽所有的可能，玩命地不断地拟合，直到达到目的。

我们把这个过程就叫做“拟合”。K8S调度控制系统也是同样的思路。

### 服务的弹性伸缩和故障迁移
有了服务状态拟合，就可以很容易的管理服务的生命周期，甚至可以通过底层的支持实现服务弹性伸缩和故障迁移。
服务伸缩所需要的操作步骤：
底层资源的伸缩；
服务的自动化部署；
服务的健康检查；
服务发现的注册；
服务流量的调度。

故障迁移，有两种模式：宠物模式、奶牛模式
所谓宠物模式，就是一定要救活，主要是对于 stateful 的服务。
而奶牛模式，就是不用救活了，重新生成一个实例。
这两种模式涉及到了：
服务的健康监控（这可能需要一个 APM 的监控）。
如果是宠物模式，需要：服务的重新启动和服务的监控报警（如果重试恢复不成功，需要人工介入）。
如果是奶牛模式，需要：服务的资源申请，服务的自动化部署，服务发现的注册，以及服务的流量调度。

需要把创痛的服务迁移到Docker和K8S上，再加上更上层对服务生命周期的控制系统的调度，可以做到完全自动化的运维架构。

### 服务工作流和编排
在分布式的服务调度中，这个工作叫做 Orchestration，国内把这个词翻译成“编排”。
传统的SOA通过ESB-企业服务总线来完成。ESB 的主要功能是服务通信路由、协议转换、服务编制和业务规则应用等。
ESB 的服务编制叫 Choreography，与我们说的 Orchestration 是不一样的。
- Orchestration 的意思是，一个服务像大脑一样来告诉大家应该怎么交互，就跟乐队的指挥一样。
- Choreography 的意思是，在各自完成专属自己的工作的基础上，怎样互相协作，就跟芭蕾舞团的舞者一样。
微服务中，希望用更加轻量的中间件来取代ESB的服务编排功能。
需要一个 API Gateway 或一个简单的消息队列来做相应的编排工作。在 Spring Cloud 中，所有的请求都统一通过 API Gateway（Zuul）来访问内部的服务。这个和 Kubernetes 中的 Ingress 相似。

## 分布式系统关键技术：流量与数据调度
流量调度很容易和服务治理相混淆。
服务治理是内部系统的事情，而流量调度可以是内部的，更是外部接入层的事。
服务治理是数据中心的事，而流量调度要做得好，应该是数据中心之外的事，也就是我们常说的边缘计算，是应该在类似于 CDN 上完成的事。

### 流量调度的主要功能 
1. 依据系统运行的情况，自动地进行流量调度，在无需人工干预的情况下，提升整个系统的稳定性；
2. 让系统应对爆品等突发事件时，在弹性计算扩缩容的较长时间窗口内或底层资源消耗殆尽的情况下，保护系统平稳运行。
还是为了提供系统架构的稳定性和HA。

流量调度系统还可以实现：
服务流控。服务发现、服务路由、服务降级、服务熔断、服务保护等。
流量控制。负载均衡、流量分配、流量控制、异地灾备（多活）等。
流量管理。协议转换、请求校验、数据缓存、数据计算等。
==这些都是一个API Gateway应该做的事情==

### 流量调度的关键技术
API Gateway要调度流量，首先是抗住流量，并且会有一些轻量的业务逻辑。
1. 高性能。API Gateway 必须使用高性能的技术，所以，也就需要使用高性能的语言。
2. 扛流量。要能扛流量，就需要使用集群技术。集群技术的关键点是在集群内的各个结点中共享数据。这就需要使用像 Paxos、Raft、Gossip 这样的通讯协议。因为 Gateway 需要部署在广域网上，所以还需要集群的分组技术。
3. 业务逻辑。API Gateway 需要有简单的业务逻辑，所以，最好是像 AWS 的 Lambda 服务一样，可以让人注入不同语言的简单业务逻辑。
4. 服务化。一个好的 API Gateway 需要能够通过 Admin API 来不停机地管理配置变更，而不是通过一个.conf 文件来人肉地修改配置。
基于上述的技术要求，目前没有满足要求的API Gateway。

### 状态数据调度
对于服务调度，最难的就是有状态的服务，状态为State，也就是服务会保存一些数据，而这些数据是不能丢失的。这些数据需要随服务一起调度。
一般来说，我们会通过“转移问题”的方法来让服务变成“无状态的服务”。也就是说，会把这些有状态的东西存储到第三方服务上，比如 Redis、MySQL、ZooKeeper，或是 NFS、Ceph 的文件系统中。
所以，我们可以看到，现在的分布式系统架构中出问题的基本都是这些存储状态的服务。因为数据存储结点在 Scale 上比较困难，所以成了一个单点的瓶颈。

### 分布式事务一致性的问题
要解决数据结点的 Scale 问题，也就是让数据服务可以像无状态的服务一样在不同的机器上进行调度，这就会涉及数据的 replication 问题。而数据 replication 则会带来数据一致性的问题，进而对性能带来严重的影响。
要解决数据不丢失的问题，只能通过数据冗余的方法，就算是数据分区，每个区也需要进行数据冗余处理。这就是数据副本。当出现某个节点的数据丢失时，可以从副本读到。数据副本是分布式系统解决数据丢失异常的唯一手段。简单来说：
数据HA-》写多份数据-》引起数据一致性问题-》引发性能问题
解决数据副本间一致性问题，技术方案：MS方案、双M方案、2PC和3PC、Paxos
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/bf378e1c20b0d770fd70474bb7039dd3_MD5.jpeg)
现在很多公司的分布式系统事务基本上都是2PL的变种。比如：阿里推出的 TCC–Try–Confirm–Cancel，或是我在亚马逊见到的 Plan–Reserve–Confirm 的方式，等等。凡是通过业务补偿，或是在业务应用层上做的分布式事务的玩法，基本上都是两阶段提交，或是两阶段提交的变种。
换句话说，迄今为止，在应用层上解决事务问题，只有“两阶段提交”这样的方式，而在数据层解决事务问题，Paxos 算法则是不二之选。

### 数据节点的分布式方案
真正完整解决数据 Scale 问题的应该还是数据结点自身。这样才能对上层透明。
问题解决应该在数据存储方，但是数据存储结果有太多的Schema，有KV、时序等，这就是分布式数据存储系统比较难做的原因。
数据存储基本上都是在解决数据副本、数据一致性和分布式事务的问题。
对于需要文件存储的，则需要分布式文件系统的支持。
==所以，真正解决数据结点调度的方案应该是底层的数据结点。==

在分布式系统里，业务层需要处理大规模的数据访问。如果数据库是单机的，肯定会遇到 **存储瓶颈（容量限制）** 和 **性能瓶颈（QPS/吞吐量限制）**。  
所以必须拆分数据存储。目前有两种方案：
1. 业务中间件层解决（如 TDDL/DAL、ShardingSphere）
2. 数据结点自身解决（分布式数据库）
像阿里的 **TDDL**、各公司的 **DAL** 或 **ShardingSphere**，解决的是“如何把 SQL 分发到不同数据库”的问题，本质上是 **在业务和存储之间加了一个路由层**。只是一个过渡方案，最终还得是靠分布式数据库。
### 状态数据调度小结
- 应用层上的分布式事务一致性，只有2PC这样的方式
- 底层存储可以通过Paxos、Raft和NWR算法解决
- 状态数据调度应该是由分布式存储系统来解决的，这样会更为完美。但是因为数据存储的 Scheme 太多，所以，导致我们有各式各样的分布式存储系统
状态数据调度应该是在 IaaS 层的数据存储解决的问题，而不是在 PaaS 层或者 SaaS 层来解决的。
在 IaaS 层上解决这个问题，一般来说有三种方案，一种是使用比较廉价的开源产品，如：NFS、Ceph、TiDB、CockroachDB、ElasticSearch、InfluxDB、MySQL Cluster 和 Redis Cluster 之类的；另一种是用云计算厂商的方案。当然，如果不差钱的话，可以使用更为昂贵的商业网络存储方案。

## 洞悉PaaS平台的本质
一家商业公司的软件工程能力体现在：
1. 提高服务的SLA，能提供多少个9的服务可用性
提高系统的SLA体现在：高可用的系统、自动化的运维
2. 能力和资源重用或复用
表现在：软件模块的重用、软件运行环境和资源的重用
也就是“软件抽象的能力”和“软件标准化的能力”
3. 过程的自动化
软件生产流水线、软件运维自动化

只有做到分布式多层的系统架构、服务化的能力供应、自动化的运维能力，才是Cloud Native。PaaS平台就是这种能力的体现。

### PaaS平台的本质
一个好的 PaaS 平台应该具有分布式、服务化、自动化部署、高可用、敏捷以及分层开放的特征，并可与 IaaS 实现良好的联动。
PaaS与传统中间件最大的差别：
- 服务化是 PaaS 的本质。软件模块重用，服务治理，对外提供能力是 PaaS 的本质。
- 分布式是 PaaS 的根本特性。多租户隔离、高可用、服务编排是 PaaS 的基本特性。
- 自动化是 PaaS 的灵魂。自动化部署安装运维，自动化伸缩调度是 PaaS 的关键。

### PaaS平台的总体架构
主要依靠Docker和K8S构建。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/6d4a208a61801fc3967a15e56aede7f8_MD5.jpeg)
在 Docker+Kubernetes 层之上，我们看到了两个相关的 PaaS 层。一个是 PaaS 调度层，很多人将其称为 iPaaS；另一个是 PaaS 能力层，通常被称为 aPaaS。没有 PaaS 调度层，PaaS 能力层很难被管理和运维，而没有 PaaS 能力层，PaaS 就失去了提供实际能力的业务价值。
在两个相关的 PaaS 层之上，有一个流量调度的接入模块，这也是 PaaS 中非常关键的东西。流控、路由、降级、灰度、聚合、串联等等都在这里，包括最新的 AWS Lambda Service 的小函数等也可以放在这里。这个模块应该是像 CDN 那样来部署的。
然后，在这个图的两边分别是与运营和运维相关的。运营这边主要是管理一些软件资源方面的东西（类似 Docker Hub 和 CMDB），以及外部接入和开放平台上的东西，这主要是对外提供能力的相关组件；而运维这边主要是对内的相关东西，主要就是 DevOps。
总结一下，一个完整的 PaaS 平台会包括以下几部分：
- PaaS 调度层 – 主要是 PaaS 的自动化和分布式对于高可用高性能的管理。
- PaaS 能力服务层 – 主要是 PaaS 真正提供给用户的服务和能力。
- PaaS 的流量调度 – 主要是与流量调度相关的东西，包括对高并发的管理。
- PaaS 的运营管理 – 软件资源库、软件接入、认证和开放平台门户。
- PaaS 的运维管理 – 主要是 DevOps 相关的东西。

### PaaS平台的生产和运维
下图是软件生产、运维和服务接入的流程：
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/160d9686c3ea6ee0b4cbb7e1cb9dc27c_MD5.jpeg)
从左上开始软件构建，进入软件资产库（Docker Registry+ 一些软件的定义），然后走 DevOps 的流程，通过整体架构控制器进入生产环境，生产环境通过控制器操作 Docker+Kubernetes 集群进行软件部署和生产变更。
其中，同步服务的运行状态，并通过生命周期管理来拟合状态，如图右侧部分所示。服务运行时的数据会进入到相关应用监控，应用监控中的一些监控事件会同步到生命周期管理中，再由生命周期管理器来做出决定，通过控制器来调度服务运行。当应用监控中心发现流量变化，要进行强制性伸缩时，它通过生命周期管理来通知控制系统进行伸缩。
左下是服务接入的相关组件，主要是网关服务，以及 API 聚合编排和流程处理。这对应于之前说过的流量调度和 API Gateway 的相关功能。



