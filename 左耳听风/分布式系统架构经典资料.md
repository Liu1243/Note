[28 \| 推荐阅读：分布式系统架构经典资料 - 极客时间已完结课程限时免费阅读](https://freegeektime.com/100002201/2080/)
[左耳朵耗子推荐：分布式系统架构经典资料\_语言 & 开发\_陈皓\_InfoQ精选文章](https://www.infoq.cn/article/2018/05/distributed-system-architecture)

## 基础理论

### CAP定理
分布式数据存储不可能同时满足以下三个条件：
- 一致性（Consistency）：每次读取要么获得最近写入的数据，要么获得一个错误。
- 可用性（Availability）：每次请求都能获得一个（非错误）响应，但不保证返回的是最新写入的数据。
- 分区容忍（Partition tolerance）：尽管任意数量的消息被节点间的网络丢失（或延迟），系统仍继续运行。
CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。需要结合实际业务场景来做trade off。
例如，对于大多数互联网应用来说（如门户网站），因为机器数量庞大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。而对于银行等，需要确保一致性的场景，通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/977521f7dcecc84bc37fec1b60a6330d_MD5.jpeg)
CA (consistency + availability)，这样的系统关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。
CP (consistency + partition tolerance)，这样的系统关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法（Quorum 类的算法）。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。
AP (availability + partition tolerance)，这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/e2aff59044d933e820983a6bc93c1ff8_MD5.jpeg)

### 关键点
失败和时间（Failure and Time）：
进程可能会失败，没有好方法表明进程失败。这就涉及到如何设置系统时钟，以及进程间的通讯机制，在没有任何共享时钟的情况下，如何确定一个事件发生在另一个事件之前。
参考Lamport时钟和Vector时钟。
> Lamport时钟：
> 每个进程维护一个整数计数器 `LC`（Lamport Clock）。
> 1. **本地事件发生时**：`LC = LC + 1`
> 2. **发送消息时**：附带当前 `LC` 作为时间戳
> 3. **接收消息时**：LC = max(LC, 接收到的LC) + 1
> 如果事件 A **happens-before** 事件 B，则 `LC(A) < LC(B)`。反之不成立，一些并发时间可能被错误的线性化。
> ==**Lamport 时钟能保证因果关系的一致性，但不能区分并发事件**。==

> Vector时钟：
> 为了解决 Lamport 时钟不能区分并发事件的问题。
> 每个进程维护一个 **向量时钟 VC**，长度等于系统中进程的数量。其中`VC[i]` 表示该进程已知的 **进程 i 的逻辑时钟**。
> 1. **本地事件**：`VC[self] += 1`
> 2. **发送消息**：携带整个 `VC`
> 3. **接收消息**：`VC = max(VC_local, VC_remote)`，然后 `VC[self] += 1`
> 若 `VC(A) ≤ VC(B)` 且至少一个分量 `<`，则 A → B（A 先于 B）。
> 若 `VC(A)` 和 `VC(B)` 不可比较（有些分量大，有些分量小），则 A 与 B **并发**。
> ==**Vector 时钟既能捕捉因果关系，也能识别并发性**。==

容错的压力（The basic tension of fault tolerance）：
能在不降级的情况下容错的系统一定要像没有错误发生的那样运行。这就意味着，系统的某些部分必须冗余地工作，从而在性能和资源消耗两方面带来成本。
最终一致性以及其他技术方案在以系统行为弱保证为代价，来试图避免这种系统压力。

基本原语（Basic primitives）：
在分布式系统中几乎没有一致认同的基本构建模块，但目前在越来越多地在出现。
例如Leader选举、分布式状态机复制

基本结论（Fundamental Results）：
如果进程之间可能丢失某些消息，那么不可能在实现一致性存储的同时响应所有的请求，这就是 CAP 定理；
一致性不可能同时满足以下条件：a. 总是正确，b. 在异步系统中只要有一台机器发生故障，系统总是能终止运行——停止失败（FLP 不可能性）；
> 既然 FLP 定理告诉我们“在纯异步系统里没救”，那工程实践怎么办？
> **放宽假设**：假设“同步”最终会恢复（即 _部分同步模型_）。这就是 Paxos、Raft 的基础。
> **放宽要求**：允许偶尔违反一致性（如 **CAP 定理里的 AP 系统**：Cassandra、Dynamo）。
> **使用随机化算法**：如随机超时的 leader 选举（Raft），规避“确定性”约束。

一般而言，消息交互少于两轮都不可能达成共识（Consensus）。

真实系统（Real systems）：
结合一些真实系统的描述，反复思考和点评其背后的设计决策。如谷歌的 GFS、Spanner、Chubby、BigTable、Dapper 等，以及 Dryad、Cassandra 和 Ceph 等非谷歌系统。

### FLP不可能性
是关于理论上能做出的功能最强的共识算法会受到怎样的限制的讨论。
共识问题，就是让网络上的分布式处理者最后都对同一个结果值达成共识。在同步环境下，每个操作步骤的时间和网络通信的延迟都是有限的，要解决共识问题是可能的。
共识问题有几个变种，它们在“强度”方面有所不同——通常，一个更“强”问题的解决方案同时也能解决比该问题更“弱”的问题。共识问题的一个较强的形式如下：
给出一个处理者的集合，其中每一个处理者都有一个初始值，
- 所有无错误的进程（处理过程）最终都将决定一个值；
- 所有会做决定的无错误进程决定的都将是同一个值；
- 最终被决定的值必须被至少一个进程提出过。
三个特性被称为“终止”、“一致同意”和“有效性”。任何一个具备这三点特性的算法都被认为是解决了共识问题。

FLP 不可能性则讨论了异步模型下的情况，主要结论有两条。
1. 在异步模型下不存在一个完全正确的共识算法。
2. 在异步模型下存在一个部分正确的共识算法，前提是所有无错误的进程都总能做出一个决定，此外没有进程会在它的执行过程中死亡，并且初始情况下超过半数进程都是存活状态。
FLP 的结论是，在异步模型中，仅一个处理者可能崩溃的情况下，就已经没有分布式算法能解决共识问题。这是该问题的理论上界。其背后的原因在于，异步模型下对于一个处理者完成工作然后再回复消息所需的时间并没有上界。因此，无法判断出一个处理者到底是崩溃了，还是在用较长的时间来回复，或者是网络有很大的延迟。

FLP的启发：网络延迟很重要、计算时间也很重要
像 Paxos 这样的共识算法为什么可行？实际上它并不属于 FLP 不可能性证明中所说的“完全正确”的算法。它的正确性会受超时值的影响。但这并不妨碍它在实践中有效，因为我们可以通过避免网络拥塞等手段来保证超时值是合适的。



