## 缓存
分布式系统中最耗性能的是后端数据库，一般insert、update和delete不会出现性能问题。
绝大多数情况，select会出现性能问题。一方面，select 会有很多像 join、group、order、like 等这样丰富的语义；另一方面，大多数应用都是读多写少，所以加剧了慢查询的问题。
使用缓存是非常必要的事情。

缓存的三种模式：

### Cache Aside更新模式
最常用的设计模式。
	失效：应用程序先从 Cache 取数据，如果没有得到，则从数据库中取数据，成功后，放到缓存中。
	命中：应用程序从 Cache 中取数据，取到后返回。
	更新：先把数据存到数据库中，成功后，再让缓存失效。
为什么不是写完数据库后更新缓存？主要是怕两个并发的写操作导致脏数据。
要么通过 2PC 或是 Paxos 协议保证一致性，要么就是拼命地降低并发时脏数据的概率。而 Facebook 使用了这个降低概率的玩法，因为 2PC 太慢，而 Paxos 太复杂。当然，最好还是为缓存设置好过期时间。

### Read/Write Through更新模式
Read/Write Through 套路是把更新数据库（repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。应用认为后端就是一个单一的存储，而存储自己维护自己的 Cache。

Read Through
Read Through 套路就是在查询操作中更新缓存

Write Through
命中了缓存，则更新缓存，然后由 Cache 自己更新数据库（同步操作）

### Write Behind Caching更新模式
Write Behind 又叫 Write Back。与Linux文件系统的page cache算法一样。
Write Back 套路就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的 I/O 操作飞快无比（因为直接操作内存嘛）。因为异步，Write Back 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。
但数据不是强一致性的，而且可能会丢失。
Write Back 实现逻辑比较复杂，因为它需要 track 有哪些数据是被更新了的，需要刷到持久层上。

### 缓存设计的重点
缓存是通过牺牲强一致性来提高性能的，使用缓存的时候，一般会使用 LRU 策略。LRU 在读写时都需要加锁（除非是单线程无并发），因此 LRU 可能会导致更慢的缓存存取的时间。这点要小心。

## 异步处理
异步通讯在分布式系统中还可以增加整个系统的吞吐量，从而可以面对更高的并发，并可以从容地利用好现有的系统资源。
==这就是异步系统所带来的好处——让我们的系统可以统一调度。==

### 异步处理的设计
异步通讯讲的是怎么把系统连接起来，而异步处理处理的是任务。
我们收到请求后，给客户端返回“收到请求，正在处理中”。然后，我们有个任务处理系统来真正地处理收到的这些请求。为了解耦，我们需要一个任务派发器，这里就会出来两个事，一个是推模型 Push，一个是拉模型 Pull。
一般来说，Push 模型可以做调度，但是它需要知道下游工作结点的情况。而 Pull 的好处则是可以让上游结点不用关心下游结点的状态，只要自己忙得过来，就会来拿任务处理，这样可以减少一定的复杂度，但是少了整体任务调度。
一般来说，我们构建的都是推拉结合的系统，Push 端会做一定的任务调度，比如它可以像物流那样把相同商品的订单都合并起来，打成一个包，交给下游系统让其一次处理掉；也可以把同一个用户的订单中的不同商品给拆成多个订单。然后 Pull 端来订阅 Push 端发出来的异步消息，处理相应的任务。

### 事件溯源
所谓 Event Sourcing，其主要想解决的问题是，我们可以看到数据库中的一个数据的值（状态），但我们完全不知道这个值是怎么得出来的。
只需要追加不可修改的数据操作事件，而不是保存最终状态。除了可以提高性能和响应时间之外，还可以提供事务数据一致性，并保留了可以启用补偿操作的完整记录和历史记录。
事件不可变，并且可使用只追加操作进行存储。 用户界面、工作流或启动事件的进程可继续，处理事件的任务可在后台异步运行。 此外，处理事务期间不存在争用，这两点可极大提高应用程序的性能和可伸缩性。
事件溯源不需要直接更新数据存储中的对象，因而有助于防止并发更新造成冲突。
最重要的是，异步处理 + 事件溯源的方式，可以很好地让我们的整个系统进行任务的统筹安排、批量处理，可以让整体处理过程达到性能和资源的最大化利用。
关于 Event Sourcing 一般会和 CQRS 一起提。
需要做snapshot，否则系统重启重放日志太恐怖。

> CQRS（Command Query Responsibility Segregation）核心思想：
> - 将 **命令（Command）** 与 **查询（Query）** 的责任分离。
> - **命令端**（写模型）：处理写操作，通常结合 Event Sourcing 只负责产生事件，不直接更新数据库。
> - **查询端**（读模型）：基于事件流或者快照，异步更新一个专门的查询数据库，用来支持高性能的读取。

### 异步处理的分布式事务
对于分布式事务，在强一致性下，在业务层上只能做两阶段提交，而在数据层面上需要使用 Raft/Paxos 的算法。但是，我想说，在现实生活中，需要用到强一致性的场景实在不多，不是所有的场景都必须要强一致性的事务的。
异步的方式可以达到最终一致性。在达成这个事务的过程中，有几点需要注意：
	凭证需要非常好地保存起来，不然会导致事务做不下去。
	凭证处理的幂等性问题，不然在重试时就会出现多次交易的情况。
	如果事务完成不了，需要做补偿事务处理。

### 异步处理的设计要点
异步处理中的事件驱动和事件溯源是两个比较关键的技术。
异步处理可能会因为一些故障导致我们的一些任务没有被处理，比如消息丢失，没有通知到，或通知到了，没有处理。有这一系列的问题，需要接收方回复ack。
发起方有定时器，对超时未ack的任务重新发起。要求接收方支持幂等性处理。
异步处理整体业务事务，如果失败需要回滚，需要走补偿事务的流程。
并不是所有的业务都可以用异步的方式，比如一些需要强一致性的业务，使用异步的方式可能就不适合，这里需要我们小心地分析业务。
在运维时，我们要监控任务队列里的任务积压情况。如果有任务积压了，要能做到快速地扩容。如果不能扩容，而且任务积压太多，可能会导致整个系统挂掉，那么就要开始对前端流量进行限流。
异步处理系统的本质是把被动的任务处理变成主动的任务处理，其本质是在对任务进行调度和统筹管理。

## 数据库拓展
### 读写分离CQRS
读写分离针对读多写少的业务场景。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/5fdfe22023bfc609a3e13596f7cd4cda_MD5.jpeg)
好处是：
- 容易实现。数据库的master-slave的配置和服务框架的读写分离比较成熟。
- 可以很好地把各个业务隔离开来。不会因为一个业务把数据库拖死而导致所有的业务都死掉。
- 可以很好分担数据库读负载，读操作是最耗CPU的操作
不好的点是：
- 写库有单点故障问题。对于交易型的业务，要得到高的写操作速度，这样的方式不行。
- 数据库同步不实时，需要强一致性的读写操作还是需要落在写库上。
这种方案主要是为了减少读操作的压力。

CQRS 全称 Command and Query Responsibility Segregation，也就是命令与查询职责分离。其原理是，用户对于一个应用的操作可以分成两种，一种是 Command 也就是我们的写操作（增，删，改），另一种是 Query 操作（查），也就是读操作。Query 操作基本上是在做数据整合显现，而 Command 操作这边会有更重的业务逻辑。分离开这两种操作可以在语义上做好区分。
	命令 Command 不会返回结果数据，只会返回执行状态，但会改变数据。
	查询 Query 会返回结果数据，但是不会改变数据，对系统没有副作用。
带来的好处：
- 分工明确
- 将业务上的命令和查询职责分离，提高系统的性能和可拓展性。
- 可以看到哪些行为导致了系统的状态变化
- 从数据驱动Data-Driven转到任务驱动Task-Driven以及事件驱动
如果把 Command 操作变成 Event Sourcing，那么只需要记录不可修改的事件，并通过回溯事件得到数据的状态。于是，我们可以把写操作给完全简化掉，也变成无状态的，这样可以大幅度降低整个系统的副作用，并可以得到更大的并发和性能。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/10692d453f02ee35a6f2fa238a1ae027_MD5.jpeg)

### 分库分表
影响数据库最大的性能问题有两个，一个是对数据库的操作，一个是数据库中数据的大小。
前者业务上优化。一方面，简化业务，不要在数据库上做太多的关联查询，而对于一些更为复杂的用于做报表或是搜索的数据库操作，应该把其移到更适合的地方。比如，用 ElasticSearch 来做查询，用 Hadoop 或别的数据分析软件来做报表分析。
对于后者，如果数据库里的数据越来越多，那么也会影响我们的数据操作。而且，对于我们的分布式系统来说，后端服务都可以做成分布式的，而数据库最好也是可以拆开成分布式的。读写分离也因为数据库里的数据太多而变慢，于是，分库分表就成了我们必须用的手段。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/0aee9d08673be747643d6ced58861a92_MD5.jpeg)
关键是分库的策略以及数据访问层的中间件。
分库的策略：按地理位置、按日期、按范围或者hash算法。
数据访问层：做数据路由，要有解析SQL的能力，还要根据解析的SQL做路由。

为了避免数据访问层的麻烦，分片策略一般如下：
- 按照多租户的方式。用租户ID分，把租户隔离开来
- 按数据的种类分。
- 通过范围来分。这样分片，可以保证在同一分片中的数据是连续的，于是我们数据库操作，比如分页查询会更高效一些。一般来说，大多数情况是用时间来分片的
- 通过hash算法分。目的是降低形成热点的可能性，但是，这会带来两个问题，一个就是前面所说的跨库跨表的查询和事务问题，另一个就是如果要扩容需要重新哈希部分或全部数据。
请注意：  
	数据库分片必须考虑业务，从业务的角度入手，而不是从技术的角度入手，如果你不清楚业务，那么无法做出好的分片策略。
	请只考虑业务分片。请不要走哈希散列的分片方式，除非有个人拿着刀把你逼到墙角，你马上就有生命危险，你才能走哈希散列的分片方式。

### 数据库拓展的设计重点
首先，你需要把数据库和应用服务一同拆开。数据库就会被 " 天生地 " 给拆成服务化的，而不是一个单体的库。
真正治本的方法是数据库和服务一起拆解。
当数据库也服务化后，我们才会在这个小的服务数据库上进行读写分离或分片的方式来获得更多的性能和吞吐量。这是整个设计模式的原则——==先做服务化拆分，再做分片。==

对于分片来说，有两种分片模式，一种是水平分片，一种是垂直分片。
所说的Sharding更多是水平分片，需要注意：
	随着数据库中数据的变化，我们有可能需要定期重新平衡分片，以保证均匀分布并降低形成热点的可能性。
	分片是静态的，而数据的访问则是不可预期的，可能需要经常性地调整我们的分片，这样一来成本太高。最耗使用索引表的方式来进行分片，参考BigTable。
	如果程序必须要从多个分片检索数据的查询，则可以使用并行任务从各个分片上提取此数据，然后聚合到单个结果中。
	数据分片后，我们很难在分片之间保持引用完整性和一致性，也就是所谓的跨分片的事务，因此应尽量减少会影响多个分片中的数据的操作。如果必须跨分片修改数据，评估一致性进行2PC。
	配置和管理大量分片可能是一个挑战。

## 秒杀
### 秒杀的流程
1. 首先，你需要一个秒杀的 landing page，在这个秒杀页上有一个倒计时的按钮。
2. 一旦这个倒计时的时间到了，按钮就被点亮，让你可以点击按钮下单。
3. 一般来说下单时需要你填写一个校验码，以防止是机器来抢。
从技术上来说，这个倒计时按钮上的时间和按钮可以被点击的时间是需要后台服务器来校准的，这意味着：
前端不段轮询后端校准时间。

### 秒杀的技术挑战
我们的技术上的挑战就是怎么应对这 100 万人同时下单请求？一方面是带宽，另一方面是需要多的机器。
但是最恐怖的是，所有的请求都会集中在同一条数据库记录上，无论是怎么分库分表，还是使用了分布式数据库都无济于事，因为你面对的是单条的热点数据。

### 秒杀的解决方案
引入CDN，CDN边缘节点分担用户请求。
把小服务部署在CDN上，向前端校准时间，统计在线人数，定时回传数据中心。
数据中心向CDN传递一个概率值，秒杀开始，CDN上的小服务将概率的用户放在后面的数据中心，其余用户返回秒杀已结束。数据中心的TPS就很低。

12306不能使用这种方案，因为无法知道用户购买哪张票，无法过滤用户。最好的应对方式就是分批放票。

### 更多的思考
解决秒杀这种特定业务场景，可以使用 CDN 的边缘结点来扛流量，然后过滤用户请求（限流用户请求），来保护数据中心的系统，这样才让整个秒杀得以顺利进行。
但是双11场景，是尽可能的卖出商品。尽可能多地收订单，但又不能超过库存，其中还有大量的银行支付，各大仓库的库存查询和分配，这些都是非常慢的操作。为了保证一致性，怎么做呢？
需要认认真真地做高并发的架构和测试了，需要各个系统把自己的性能调整上去，还要小心地做性能规划，更要把分布式的弹力设计做好，最后是要不停地做性能测试，找到整个架构的系统瓶颈，然后不断地做水平扩展，以解决大规模的并发。

在数据中心解决并不一定是最好的方式，放在边缘来解决可能会更好一些。尤其是针对一些有地域特征的业务，比如像外卖、共享单车、打车这样的业务。其实，把一些简单的业务逻辑放在边缘，比放在数据中心不但能够有更好的性能，还有更便宜的成本。

## 边缘计算
### 为什么要有边缘计算
从趋势上看：整个计算机发展本质就是人类生活信息化建设的过程。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/6ab54a94aba827450e5aeaf52cc4cf41_MD5.jpeg)
我们可以看到，数量越来越大，分析结果的速度需要越来越快，这两个需求，只会把我们逼到边缘计算上去。

从成本上来说
当需要处理的数据或用户请求的规模越来越大，成本呈非线性上升的趋势。

什么样的业务可以这么做？我觉得有地域性的业务是可以这么做的，比如：外卖、叫车、共享单车之类的。

### 边缘计算的业务场景
处理一些实时响应的业务。它和用户靠得很近，所以可以实时响应用户的一些本地请求，比如，某公司的人脸门禁系统、共享单车的开锁。
处理一些简单的业务逻辑。比如像秒杀、抢红包这样的业务场景。
收集并结构化数据。比如，把视频中的车牌信息抠出来，转成文字，传回数据中心。
实时设备监控。主要是线下设备的数据采集和监控。
P2P 的一些去中心化的应用。比如：边缘结点作为一个服务发现的服务器，可以让本地设备之间进行 P2P 通讯。
云资源调度。边缘结点非常适合用来做云端服务的调度。比如，允许用户使用不同生产商的云存储服务，使用不同生产商但是功能相同的 API 服务（比如支付 API 相关）。因为是流量接入方，所以可以调度流量。
云资源聚合。比如，我们可以把语音转文字的 API 和语义识别的 API 相结合，聚合出来一个识别语音语义的 API，从而简化开发人员的开发成本。

### 边缘计算的关键技术
- API Gatewat
- Serverless/FaaS。服务函数化，就像是AWS Lambda，写好一个函数，不用关注函数运行在哪里，直接发布就可以用。
如果说微服务是以专注于单一责任与功能的小型功能块为基础，利用模块化的方式组合出复杂的大型应用程序，那么我们还可以进一步认为 Serverless 架构可以提供一种更加 " 代码碎片化 " 的软件架构范式，我们称之为 Function as a Services（FaaS）。所谓的“函数”（Function）提供的是相比微服务更加细小的程序单元。