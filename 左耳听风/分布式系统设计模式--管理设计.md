## 分布式锁
分布式系统的锁服务，一般可以使用DB、Redis和ZooKeeper实现。分布式锁服务的特点是：
	安全性（Safety）：在任意时刻，只有一个客户端可以获得锁（排他性）。
	避免死锁：客户端最终一定可以获得锁，即使锁住某个资源的客户端在释放锁之前崩溃或者网络不可达。
	容错性：只要锁服务集群中的大部分节点存活，Client 就可以进行加锁解锁操作。

### Redis分布式锁服务
通过下述命令对资源加锁：
`SET resource_name my_random_value NX PX 30000`
	my_random_value 必须是全局唯一的值。这个随机数在释放锁时保证释放锁操作的安全性。
	SET NX 命令只会在 key 不存在的时候给 key 赋值，PX 命令通知 Redis 保存这个 key 30000ms。
	获得锁的客户端如果没有在这个时间窗口内完成操作，就可能会有其他客户端获得锁，引起争用问题。
获取锁的原理就是：
只有在某个 key 不存在的情况下才能设置（set）成功该 key。于是，这就可以让多个进程并发去设置同一个 key，只有一个进程能设置成功。而其它的进程因为之前有人把 key 设置成功了，而导致失败（也就是获得锁失败）。
锁解锁通过lua脚本：
```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
	return redis.call("del",KEYS[1])
else
	return 0
end
```
通过这个方式释放锁是为了避免 Client 释放了其他 Client 申请的锁。

关于 value 的生成，官方推荐从 /dev/urandom 中取 20 个 byte 作为随机数。或者采用更加简单的方式，例如使用 RC4 加密算法在 /dev/urandom 中得到一个种子（Seed），然后生成一个伪随机流。
也可以采用更简单的方法，使用时间戳 + 客户端编号的方式生成随机数。Redis 的官方文档说：“这种方式的安全性较差一些，但对于绝大多数的场景来说已经足够安全了”。

### 分布式锁服务的一个问题
Redis的分布式锁存在问题：Redis通过超时释放锁，但是存在这样一个场景
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/30a2bd8b301028884bccd995d1d1a977_MD5.jpeg)
最后Client A会将Client B的更新覆盖。

要解决这个问题，引入fence栅栏技术，也就是乐观锁机制，通过版本号排他。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/6e473cf311cd39397fc15baacb7c7215_MD5.jpeg)
- 锁服务需要有一个单调递增的版本号。
- 写数据的时候，也需要带上自己的版本号。
- 数据库服务需要保存数据的版本号，然后对请求做检查。
如果使用ZooKeeper做分布式锁服务，可以使用zxid或znode的版本号做这个fence版本号。

### 从乐观锁到CAS
如果在数据库中存储版本号，那么完全可以使用数据库来做分布式锁服务。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/3c8439c29d51746ca6318aa1ba2475a6_MD5.jpeg)
使用数据版本（Version）记录机制，即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现的。当读取数据时，将 version 字段的值一同读出，数据每更新一次，对此 version 值加一。
当我们提交更新的时候，数据库表对应记录的当前版本信息与第一次取出来的 version 值进行比对。如果数据库表当前版本号与第一次取出来的 version 值相等，则予以更新，否则认为是过期数据。
==如果使用版本号，或者fence token的方式，就不需要使用分布式锁服务==
这种 fence token 的玩法，在数据库那边一般会用 timestamp 时间截来玩。也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则 OK，否则就是版本冲突。

有时候都不需要在数据库中增加额外的版本字段或fence token。例如想更新库存，直接对比select时库存量与需要update时的库存量进行对比，如果一致则update。这也就是CAS操作。
**从分布式锁到乐观锁，再到CAS，我们还需要分布式锁服务吗？**

### 分布式锁设计的重点
一般情况下，我们可以使用数据库、Redis 或 ZooKeeper 来做分布式锁服务，这几种方式都可以用于实现分布式锁。
分布式锁的特点是，保证在一个集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。这就是所谓的分布式互斥。所以，大家在做某个事的时候，要去一个服务上请求一个标识。如果请求到了，我们就可以操作，操作完后，把这个标识还回去，这样别的进程就可以请求到了。
分布式锁的初衷和概念性问题：
	获取锁的进程挂掉了怎么办？会导致死锁，一般是加过期时间。
	如果锁自动释放了，新进程拿到锁了，之前进程还有锁怎么办？像 Redis 那样也可以使用 Check and Set 的方式来保证数据的一致性。
	如果通过像 CAS 这样的操作的话，我们还需要分布式锁服务吗？但现实生活中也有不需要更新某个数据的场景，只是为了同步或是互斥一下不同机器上的线程，这时候像 Redis 这样的分布式锁服务就有意义了。
我是用来修改某个共享源的，还是用来不同进程间的同步或是互斥的。如果使用 CAS 这样的方式（无锁方式）来更新数据，那么我们是不需要使用分布式锁服务的，而后者可能是需要的。==所以，这是我们在决定使用分布式锁服务前需要考虑的第一个问题——我们是否需要？==
如果确定使用分布式锁服务，需要考虑：
	需要给一个锁被释放的方式，以避免请求者不把锁还回来，导致死锁的问题。Redis通过超时时间，ZooKeeper通过sessionTimeout删除节点。
	分布式锁服务应该是HA的，并且需要持久化。
	要提供非阻塞方式的锁服务。
	还要考虑锁的可重入性。
Redis 也是不错的，ZooKeeper 在使用起来需要有一些变通的方式，好在 Apache 有 Curator 帮我们封装了各种分布式锁的玩法。

> Apache Curator是对ZK的封装。
> **Curator**：由 Netflix 开发，后来捐给 Apache，提供了更高层次的抽象，解决了 ZK 的常见坑，并内置了很多 **recipes（模式/工具类）**。
> 在HA上，和 Redis RedLock 不同，Curator 的锁是基于 **ZooKeeper 的强一致性协议（ZAB 协议）** 来实现的

|特性|Redis RedLock|Curator（基于 ZK）|
|---|---|---|
|一致性|**最终一致性**（可能存在极端并发错误）|**强一致性**（ZAB 协议保证）|
|可用性|依赖多 Redis 实例，过半存活即可|依赖 ZK 集群（半数以上存活）|
|适用场景|分布式调度、缓存控制、限流等|金融、交易、强一致性要求高的业务|
|易用性|Redisson/自己实现|Curator 封装好，直接用|
|自动释放|依赖过期时间|依赖 session，断连自动清理|
### 小结
为什么需要分布式锁服务？就像单机系统上的多线程程序需要用操作系统锁或数据库锁来互斥对共享资源的访问一样，分布式程序也需要通过分布式锁来互斥对共享资源的访问。
分布式锁服务一般可以通过 Redis 和 ZooKeeper 等实现。
进一步，数据库如果本身利用 CAS 等手段支持这种版本控制方式，其实也就没必要用一个独立的分布式锁服务了。最后，我们发现，分布式锁服务还能用来做同步，这是数据库锁做不了的事情。

## 配置中心
软件会有一些配置信息，比如数据库用户名密码等，在分布式场景下，为了便于管理，会引入一个集中式的配置管理系统。

### 配置中心的设计
区分软件的配置
一种方式是把软件的配置分成静态配置和动态配置。
	所谓静态配置其实就是在软件启动时的一些配置，运行时基本不会进行修改，也可以理解为是环境或软件初始化时需要用到的配置。例如OS网络配置、Docker进程配置。
	动态配置其实就是软件运行时的一些配置，在运行时会被修改。比如，日志级别、降级开关、活动开关。
这里主要针对动态配置的管理。
动态配置有三个区分的维度：
- 按照运行环境分。开发环境、测试环境、预发环境、生产环境。
- 按照依赖区分。一种是依赖配置，一种是不依赖的内部配置。比如，外部依赖的 MySQL 或 Redis 的连接配置。还有一种完全是自己内部的配置。
- 按照层次分。就像云计算一样，配置也可以分成 IaaS、PaaS、SaaS 三层。基础层的配置是操作系统的配置，中间平台层的配置是中间件的配置，如 Tomcat 的配置，上层软件层的配置是应用自己的配置。

配置中心的模型
配置项就是key、value的模型。
IaaS层、PaaS层，也就是OS层和平台层是由专门的运维人员或者架构师配置，value应该是选项，而不是用户自由输入的，最好有相关的模板。
SaaS层，应用层的配置项，有响应的命名规范，以及命名空间，确保不同应用的配置项不会冲突。

外部服务依赖的配置应该放在服务发现系统中，而不是配置中心。这样使得语义更加清楚，也会减少因为运行不同环境导致配置不同的差异性。
对于不同运行环境中配置的差异来说，比如在开发环境和测试环境下，日志级别是 Debug 级，对于生产环境则是 Warning 或 Error 级，因为环境的不一样，会导致我们需要不同的配置项的值。
配置应该有一个整体的版本管理，每次变动将版本的差异记录下来。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/7ea666aa5321c66745e206ed6e30d6ef_MD5.jpeg)

配置中心的架构
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/f4c8885d2d2f1e0d38266865bd70287e_MD5.jpeg)
在这个图中可以看到，我们把配置录入后，配置中心发出变更通知，配置变更控制器会来读取最新的配置，然后应用配置。
	为什么需要一个变更通知的组件，而不是配置中心直接推送？分布式环境下，服务器太多，推送不太现实，而采用一个 Pub/Sub 的通知服务可以让数据交换经济一些。
	为什么不直接Pub数据过去，还要订阅方反向拉数据？直接Pub也可以。但是反向拉取的好处是API可以校验请求者的权限，而且有时候需要调用配置中心的基本API。
	配置变更控制器部署在哪里？是在每个服务器上，还是在一个中心的地方？变更配置的步骤算是一个事务，为了执行效率，建议将配置变更控制器在每个服务器都部署。
	平台层的配置变更，有的参数是在服务启动的命令行上，这个怎么变更呢？一般来说，命令行上的参数需要通过 Shell 环境变量做成配置项，然后通过更改系统环境变量，并重启服务达到配置变更。
	操作系统的配置变更和平台层的配置变更最好模块化掉，就像云服务中的不同尺寸的主机型号一样。 这样有利于维护和减少配置的复杂性。
	应用服务配置更新的标准化。主要有三种方式：SDK、运维脚本方式、Agent方式
SDK方式（开发期接入）：供一个统一的 SDK/开发框架，应用直接通过它读取配置，并支持订阅配置更新事件（observer 模式）。标准化最彻底，开发规范统一。但是耦合语言。
运维脚本方式（Ops驱动）：要求应用方提供配置变更时的运维脚本（如 `reload.sh`），平台通过控制器统一调度执行这些脚本。不耦合语言。配置变更逻辑分散在各应用，难以统一标准。并且脚本复杂容易出错。
Agent方式（Sidecar）：为每个应用部署一个 **Agent/Sidecar**，对外暴露标准 Admin API；Agent 内部再去适配不同应用的配置方式（文件修改、调用内部 API 等）。语言无关，对应用透明。需要额外运维Agent，增加应用宿主机的复杂度。

|方案|优点|缺点|适用场景|
|---|---|---|---|
|SDK|彻底标准化，开发期统一|耦合语言、改造成本高|技术栈统一，新项目建设|
|运维脚本|灵活，语言无关|脚本复杂，标准化差，易出错|老系统多，异构环境|
|Agent（Sidecar）|对外统一接口，透明，灵活落地|Agent 维护成本高|大中型公司，异构系统，需要可控扩展|

### 配置中心的设计重点
配置中心主要的用处是统一和规范化管理所有的服务配置，也算是一种配置上的治理活动。配置中心的设计重点应该放在如何统一和标准化软件的配置项，其还会涉及到软件版本、运行环境、平台、中间件等一系列的配置参数。
编程的本质是对 logic 和 control 的分离，所以，对于配置也一样，其也有控制面上的配置和业务逻辑面上的配置，控制面上的配置最好能标准统一。
配置更新的过程是一个事务，需要考虑失败回滚。
配置更新控制器，需要应用服务的配合，比如，配置的 reload，服务的优雅重启，服务的 Admin API，或是通过环境变量……这些最好是由一个统一的开发框架搞定。
配置更新控制器还担任服务启动的责任，由配置更新控制器来启动服务。这样，配置控制器会从配置中心拉取所有的配置，更新操作系统，设置好启动时用的环境变量，并更新好服务需要的配置文件 ，然后启动服务。

## 边车模式
我们不需要在服务中实现控制面上的东西，如监视、日志记录、限流、熔断、服务注册、协议适配转换等这些属于控制面上的东西，而只需要专注地做好和业务逻辑相关的代码，然后，由“边车”来实现这些与业务逻辑没有关系的控制功能。

### 边车模式设计
边车就有点像一个服务的 Agent，这个服务所有对外的进出通讯都通过这个 Agent 来完成。保证Agent和应用程序一起创建，一起停用。
边车模式有时候也叫搭档模式，或是伴侣模式，或是跟班模式。==编程的本质就是将控制和逻辑分离和解耦==，而边车模式也是异曲同工，同样是让我们在分布式架构中做到逻辑和控制分离。
对于监视、日志、限流、熔断、服务注册、协议转换等等这些功能，其实都是大同小异，甚至是完全可以做成标准化的组件和模块的。一般来说，我们有两种方式：
- SDK、Lib或Framework软件包方式，开发时与真实的应用服务集成
- Sidecar的方式，在运维时与真实的应用服务集成起来
这两种方式各有优缺点。
- 以软件包的方式可以和应用密切集成，有利于资源的利用和应用的性能，但是对应用有侵入，而且受应用的编程语言和技术限制。同时，当软件包升级的时候，需要重新编译并重新发布应用。
- Sidecar方式对应用服务没有侵入性，并且不用受到应用服务的语言和技术的限制，而且可以做到控制和逻辑的分开升级和部署。但也增加了每个应用服务的依赖性，也增加了应用的延迟，并且也会大大增加管理、托管、部署的复杂度。

对于一些很老的系统，使用Sidecar模式就很有价值了，没有侵入性，可以很快的低风险改造。
Sidecar在逻辑上和应用程序在同一个结点上，和应用服务有相同的生命周期。对比于应用程序的每个实例，都会有一个 Sidecar 的实例。Sidecar 可以很快也很方便地为应用服务进行扩展，而不需要应用服务的改造。比如：
	Sidecar 做服务发现
	Sidecar做服务路由
	Sidecar接管进出的流量，做日志监视、调用链分总、流控熔断等
	服务控制系统可以通过控制 Sidecar 来控制应用服务，如流控、下线等。
于是应用程序完全做到专注于业务逻辑。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/82927fff48f656832539fc68dc835598_MD5.jpeg)
注意，如果把 Sidecar 这个实例和应用服务部署在同一台机器中，那么，其实 Sidecar 的进程在理论上来说是可以访问应用服务的进程能访问的资源的。比如，Sidecar 是可以监控到应用服务的进程信息的。

### 边车模式设计的重点
重点解决：
	控制和逻辑的分离。
	服务调用中上下文的问题。
熔断、路由、服务发现、计量、流控、监视、重试、幂等、鉴权等控制面上的功能，以及其相关的配置更新，本质来上来说，和服务的关系并不大。
而随着系统架构的复杂化和扩张，我们需要更统一地管理和控制这些控制面上的功能，所以传统的在开发层面上完成控制面的管理会变得非常难以管理和维护。这使得我们需要通过 Sidecar 模式来架构我们的系统。
工程实现上，需要注意：
	进程间通讯机制是这个设计模式的重点，千万不要使用任何对应用服务有侵入的方式，比如，通过信号的方式，或是通过共享内存的方式。最好的方式就是网络远程调用的方式（因为都在 127.0.0.1 上通讯，所以开销并不明显）。
	服务协议方面，也请使用标准统一的方式。这里有两层协议，一个是 Sidecar 到 service 的内部协议，另一个是 Sidecar 到远端 Sidecar 或 service 的外部协议。内部使用更加兼容本地service的协议；对于外部协议，使用更加开发标准的协议。都不能与语言相关。
	使用这样的模式，需要在服务的整体打包、构建、部署、管控、运维上设计好。使用 Docker 容器方面的技术可以帮助你全面降低复杂度。
	Sidecar实现的是控制面上的东西。
	小心在 Sidecar 中包含通用功能可能带来的影响。例如，重试操作，这可能不安全，除非所有操作都是幂等的。
	另外，我们还要考虑允许应用服务和 Sidecar 的上下文传递的机制。

Sidecar适用的场景：
	对老应用系统的改造和拓展
	对多语言混合的分布式系统进行管理和拓展
	其中的应用服务由不同的供应商提供。
	把控制和逻辑分离，标准化控制面上的动作和技术，从而提高系统整体的稳定性和可用性。也有利于分工——并不是所有的程序员都可以做好控制面上的开发的。

Sidecar不适用于：
	架构不复杂，不需要这个模式。直接使用API Gateway或者Nginx和HAProxy等即可
	服务间的协议不标准且无法转换
	不需要分布式的架构

### 小结
为了把诸如监视、日志、限流等控制逻辑与业务逻辑分离解耦，我们可以采用边车模式。与之对应的另一种实现控制逻辑的方式是库或框架。虽然相对来说边车模式资源消耗较大，但控制逻辑不会侵入业务逻辑，还能适应遗留老系统的低风险改造。
边车作为另一个进程，和服务进程部署在同一个结点中，通过一个标准的网络协议，如 HTTP 来进行通信。这样可以做到低延迟和标准化。同时，用 Docker 来打包边车和服务两者，可以非常方便部署。

## 服务网格
Sidecar边车模式可以有效的分离系统控制和业务逻辑，让整个系统架构在控制面上可以集中管理。
假如，我们在一个分布式系统中，已经把一些标准的 Sidecar 给部署好了（熔断、限流、重试、幂等、路由、监视等），那么真实的业务服务只需要往这个集群中放，就可以和本地的 Sidecar 通信，然后由 Sidecar 委托代理与其它系统的交互和控制。

### 什么是Service Mesh
Service Mesh 这个服务网络专注于处理服务和服务间的通讯。其主要负责构造一个稳定可靠的服务通讯的基础设施，并让整个架构更为的先进和 Cloud Native。在工程中，Service Mesh 基本来说是一组轻量级的服务代理和应用逻辑的服务在一起，并且对于应用服务是透明的。
特点是：
	Service Mesh 是一个基础设施。
	Service Mesh 是一个轻量的服务通讯的网络代理。
	Service Mesh 对于应用服务来说是透明无侵入的。
	Service Mesh 用于解耦和分离分布式系统架构中控制层面上的东西。
Service Mesh 就像是网络七层模型中的第四层 TCP 协议。其把底层的那些非常难控制的网络通讯方面的控制面的东西都管了（比如：丢包重传、拥塞控制、流量控制），而更为上面的应用层的协议，只需要关心自己业务应用层上的事了。如 HTTP 的 HTML 协议。

Service Mesh的演化路径：
- 两台主机间进程直接通信
- 分离出网络层，服务间远程通信，通过底层的网络模型完成
- 因为两边服务接收速度不一致，应用层实现流控
- 流控模块交给网络层实现，TCP/IP模型出现
- 意识到分布式系统需要“弹力设计”，在更上层加入了限流、熔断、服务发现、监控等功能
- 弹力设立的设计模式可以标准化，写成SDK/Lib/Framework，在开发层面集成到应用服务中
- SDK等不能跨语言，需要一个专门的层负责，出现Sidecar
- Sidecar集群成了Service Mesh
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/fdb743e1e139b8fac4a38e2fecae7bff_MD5.jpeg)
图中的绿色模块是真实的业务应用服务，蓝色模块则是 Sidecar，其组成了一个网格。而我们的应用服务完全独立自包含，只需要和本机的 Sidecar 依赖，剩下的事全交给了 Sidecar。

### Service Mesh相关开源软件
目前比较流行的 Service Mesh 开源软件是 Istio 和 Linkerd，它们都可以在 Kubernetes 中集成。当然，还有一个新成员 Conduit，它是由 Linkerd 的作者出来自己搞的，由 Rust 和 Go 写成的。Rust 负责数据层面，Go 负责控制面。号称吸取了很多 Linkerd 的 Scala 的教训，比 Linkerd 更快，还轻，更简单。
lstio 是目前最主流的解决方案，其架构并不复杂，其核心的 Sidecar 被叫做 Envoy（使者），用来协调服务网格中所有服务的出入站流量，并提供服务发现、负载均衡、限流熔断等能力，还可以收集大量与流量相关的性能指标。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/ecf7f2c0cfb9c4efbb1a01b17bd2f729_MD5.jpeg)

### Service Mesh的设计重点
如果 Service Mesh 所管理的 Sidecar 出了问题，那应该怎么办？所以，Service Mesh 这个网格一定要是高可靠的，或者是出现了故障有 workaround 的方式。一种比较好的方式是，除了在本机有 Sidecar，我们还可以部署一下稍微集中一点的 Sidecar——比如为某个服务集群部署一个集中式的 Sidecar。一旦本机的有问题，可以走集中的。
这样一来，Sidecar 本来就是用来调度流量的，而且其粒度可以细到每个服务的实例，可以粗到一组服务，还可以粗到整体接入。这看来看去都像是一个 Gateway 的事。所以，我相信，使用 Gateway 来干这个事应该是最合适不过的了。
Service Mesh 不像 Sidecar 需要和 Service 一起打包一起部署，Service Mesh 完全独立部署。这样一来，Service Mesh 就成了一个基础设施，就像一个 PaaS 平台。所以，Service Mesh 能不能和 Kubernetes 密切结合就成为了非常关键的因素。

## 网关模式
Sidecar和Service Mesh这两种设计模式，是在不侵入业务逻辑的情况下，把控制面和数据面的处理解耦分离。但这两种模式会使运维成本变大，因为每个服务都需要一个Sidecar。
我个人觉得并不需要为每个服务的实例都配置上一个 Sidecar。其实，一个服务集群配上一个 Gateway 就可以了，或是一组类似的服务配置上一个 Gateway。
Gateway方式下的架构，粗细度就可以自己配置。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/ad2f7087d94599a4c66585e8a6f51b06_MD5.jpeg)

### 网关设计模式
一个网关需要功能：
	请求路由。因为不再是 Sidecar 了，所以网关一定要有请求路由的功能。
	服务注册。
	负载均衡。简单点Round-Robin，复杂点设置权重，在复杂点做到session粘连。
	弹力设计。异步、重试、幂等、流控、熔断、监视等的实现。这样，同样可以像 Service Mesh 那样，让应用服务只关心自己的业务逻辑（或是说数据面上的事）而不是控制逻辑（控制面）。
	安全方面。SSL 加密及证书管理、Session 验证、授权、数据校验，以及对请求源进行恶意攻击的防范。
网关还可以做：
	灰度发布。
	API聚合。使用网关可以将多个单独请求聚合成一个请求。
	API编排。通过一个 DSL 来定义和编排不同的 API，也可以通过像 AWS Lambda 服务那样的方式来串联不同的 API。

### Gateway、Sidecar和Service Mesh
Sidecar 的方式主要是用来改造已有服务。
当 Sidecar 在架构中越来越多时，需要我们对 Sidecar 进行统一的管理。于是，我们为 Sidecar 增加了一个全局的中心控制器，就出现了我们的 Service Mesh。
然而，Service Mesh 的架构和部署太过于复杂，会让我们运维层面上的复杂度变大。为了简化这个架构的复杂度，我认为 Sidecar 的粒度应该是可粗可细的，这样更为方便。但我认为，Gateway 更为适合，而且 Gateway 只负责进入的请求，不像 Sidecar 还需要负责对外的请求。对外的请求交给其他的Gateway。
总而言之，我觉得 Gateway 的方式比 Sidecar 和 Service Mesh 更好。

|对比项|**Gateway**|**Sidecar**|**Service Mesh**|
|---|---|---|---|
|位置|系统对外入口|Pod 内部，伴随服务|整个集群服务间通信|
|关注点|**东西向流量（外部→内部）**|单服务的增强能力|**南北向+东西向（服务间）流量治理**|
|实现方式|独立服务或代理|边车容器/进程|Sidecar + 控制平面|
|功能|API 聚合、协议转换、安全控制|日志、流量代理、安全、治理|全局治理（发现、流量控制、监控、安全）|
|典型产品|Kong、APISIX、Istio Gateway|Envoy Sidecar、Fluent Bit Sidecar|Istio、Linkerd、Consul Connect|

### 网关的设计重点
1. 高性能。使用高性能的编程语言来实现，使用异步非阻塞IO实现。C 和 C++ 可以参看 Linux 下的 epoll 和 Windows 的 I/O Completion Port 的异步 IO 模型，Java 下如 Netty、Vert.x、Spring Reactor 的 NIO 框架。当然，我还是更喜欢 Go 语言的 goroutine 加 channel 玩法。
2. 高可用。做到集群化、服务化、持续化。
3. 高拓展。应该做成像 AWS Lambda 那样的方式，也就是所谓的 Serverless 或 FaaS（Function as a Service）那样的方式。

运维方面，网关的设计原则：
- 业务松耦合，协议紧耦合。网关是在网络应用层上的组件，不处理协议体，只处理协议头。
- 应用监视，提供分析数据。
- 用弹力设计保护后端服务。
- DevOps。

整体架构方面，需要注意：
- 不要在网关的代码里内置聚合后端服务的功能。通过Plugin或网关后面形成一个Serverless服务
- 网关应该靠近后端服务，并和后端服务使用同一个内网；网关处理的静态内容应该靠近用户
- 网关需要容量拓展，成为一个集群分担流量。通过DNS轮询或CDN流量调度，或者更为底层的负载均衡设备
- 服务发现，可以做一个时间不长的缓存，不用每次请求都去查服务所在地。
- 为网关考虑bulkhead设计方式。不通网关服务不同后端服务。

网关也需要考虑一些安全：
- 加密数据。SSL证书放在王观赏，网关做统一的SSL传输管理
- 校验用户的请求。基本的用户验证放在网关上做
- 检测异常访问。

## 部署升级策略
服务部署的模式：
	停机部署（Big Bang / Recreate）： 把现有版本的服务停机，然后部署新的版本。
	蓝绿部署（Blue/Green /Stage）：部署好新版本后，把流量从老服务那边切过来。
	滚动部署（Rolling Update / Ramped）： 一点一点地升级现有的服务。
	灰度部署（Canary）：把一部分用户切到新版本上来，然后看一下有没有问题。如果没有问题就继续扩大升级，直到全部升级完成。
	AB 测试（A/B Testing）：同时上线两个版本，然后做相关的比较。

### 停机部署
有时候，我们不得不使用这样的方式来部署或升级多个服务。比如，新版本中的服务使用到了和老版本完全不兼容的数据表设计。服务和数据库，新老版本都不兼容，只能使用停机部署的方式。
优势是，在部署过程中不会出现新老版本同时在线的情况，所有状态完全一致。
问题是会停机，对用户的影响很大。一般来说需要事前挂公告。

### 蓝绿部署
在生产线上部署相同数量的新服务，然后当新的服务测试确认 OK 后，把流量切到新的服务这边来。蓝绿部署比停机部署好的地方是，它无需停机。
也就是预发环境。生产线上有两套相同的集群，一套是 Prod 是真实服务的，另一套是 Stage 是预发环境，发布发 Stage，然后把流量切到 Stage 这边，于是 Stage 就成了 Prod，而之前的 Prod 则成了 Stage。
优点是没有停机，实时发布和升级，也避免有新旧版本同时在线的问题。
问题是浪费资源，需要双倍的资源。（云计算时代没事，虚拟机部署完可以释放）
==如果我们的服务中有状态，比如一些缓存什么的，停机部署和蓝绿部署都会有问题。==

### 滚动部署
滚动部署策略是指通过逐个替换应用的所有实例，来缓慢发布应用的一个新版本。
这种方式直接对现有的服务进行升级，对于有状态的服务友好。
问题是：
	在发布过程中，会出现新老两个版本同时在线的情况，同一用户的请求可能在新老版中切换而导致问题。
	我们的新版程序没有在生产线上经过验证就上线了。
	在整个过程中，生产环境处于一个新老更替的中间状态，如果有问题要回滚就有点麻烦了。
	如果在升级过程中，需要做别的一些运维工作，我们还要判断哪些结点是老版本的，哪些结点是新版本的。这太痛苦了。
	因为新老版本的代码同时在线，所以其依赖的服务需要同时处理两个版本的请求，这可能会带来兼容性问题。
	而且，我们无法让流量在新老版本中切换。

### 灰度部署（金丝雀）
灰度部署是指逐渐将生产环境流量从老版本切换到新版本。通常流量是按比例分配的。逐步扩大新版本上的流量，减少老版本上的流量。
对于多租户的平台，例如云计算平台，灰度部署也可以将一些新的版本先部署到一些用户上，如果没有问题，扩大部署，直到全部用户。一般的策略是，从内部用户开始，然后是一般用户，最后是大客户。
这个技术大多数用于缺少足够测试，或者缺少可靠测试，或者对新版本的稳定性缺乏信心的情况下。

### AB测试
AB 测试是同时上线两个版本，然后做相关的比较。它是用来测试应用功能表现的方法，例如可用性、受欢迎程度、可见性等。推荐系统常用。
蓝绿部署是为了不停机，灰度部署是对新版本的质量没信心。而 AB 测试是对新版的功能没信心。注意，一个是质量，一个是功能。
AB 测试旨在通过科学的实验设计、采样样本代表性、流量分割与小流量测试等方式来获得具有代表性的实验结论，并确信该结论在推广到全部流量时可信。
对于灰度发布或是 AB 测试可以使用下面的技术来选择用户：
	浏览器 cookie。
	查询参数。
	地理位置。
	技术支持，如浏览器版本、屏幕尺寸、操作系统等。
	客户端语言。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/6591d4506246a8dfc086a1659e9eb5f4_MD5.jpeg)