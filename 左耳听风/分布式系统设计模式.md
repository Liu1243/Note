- **容错设计又叫弹力设计**，其中着眼于分布式系统的各种“容忍”能力，包括容错能力（服务隔离、异步调用、请求幂等性）、可伸缩性（有/无状态的服务）、一致性（补偿事务、重试）、应对大流量的能力（熔断、降级）。可以看到，在确保系统正确性的前提下，系统的可用性是弹力设计保障的重点。
- **管理篇**会讲述一些管理分布式系统架构的一些设计模式，比如网关方面的，边车模式，还有一些刚刚开始流行的，如Service Mesh相关的设计模式。
- **性能设计篇**会讲述一些缓存、CQRS、索引表、优先级队列、业务分片等相关的架构模式。

## 弹力设计篇之“认识故障和弹力设计”
[41 弹力设计：认识故障和弹力设计 - 极客时间文档](https://uaxe.github.io/geektime-docs/%E5%90%8E%E7%AB%AF-%E6%9E%B6%E6%9E%84/%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/41%20-%20%E5%BC%B9%E5%8A%9B%E8%AE%BE%E8%AE%A1%EF%BC%9A%E8%AE%A4%E8%AF%86%E6%95%85%E9%9A%9C%E5%92%8C%E5%BC%B9%E5%8A%9B%E8%AE%BE%E8%AE%A1/)
### 系统可用性度量
容错设计，在英文中又叫Resiliency（弹力）。意思是，系统在不健康、不顺，甚至出错的情况下有能力hold得住，挺得住，还有能在这种逆境下力挽狂澜的能力。
$$Availability=\frac{MTTF}{MTTF +MTTR}$$
- MTTF 是 Mean Time To Failure，平均故障前的时间，即系统平均能够正常运行多长时间才发生一次故障。系统的可靠性越高，MTTF越长。（注意：从字面上来说，看上去有Failure的字样，但其实是正常运行的时间。）
- MTTR 是 Mean Time To Recovery，平均修复时间，即从故障出现到故障修复的这段时间，这段时间越短越好。
这个公式就是计算系统可用性的，也就是我们常说的，多少个9
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/cbbfa8f48d00644ae2ce70f227b049a6_MD5.jpeg)
提高可用性：要么提高系统的无故障时间，要么减少系统的故障恢复时间

### 故障原因
除了软件设计，还有硬件，还有第三方服务（如电信联通的宽带SLA），当然包括“建筑施工队的挖掘机”。
SLA不只是一个技术指标，还是服务提供商和用户之间的contract，工业界中会把服务不可用的因素分为两种：
**无计划的**
- 系统级故障，包括主机、操作系统、中间件、数据库、网络、电源以及外围设备。
- 数据和中介的故障，包括人员误操作、硬盘故障、数据乱了。
- 还有自然灾害、人为破坏，以及供电问题等。
**有计划的**
- 日常任务：备份，容量规划，用户和安全管理，后台批处理应用。
- 运维相关：数据库维护、应用维护、中间件维护、操作系统维护、网络维护。
- 升级相关：数据库、应用、中间件、操作系统、网络，包括硬件升级。

归类就是：安全问题、性能问题、网络问题、运维问题、管理问题、硬件问题

### 故障不可避免
要意识到：
- 故障是正常的，而且是常见的。
- 故障是不可预测突发的，而且相当难缠。
不要尝试着去避免故障，而是要把处理故障的代码当成正常的功能做在架构里写在代码里。
我们要干的事儿就是想尽一切手段来降低 MTTR——故障的修复时间。
这就是为什么我们把这个设计叫做弹力（Resiliency）。
- 一方面，在好的情况下，这个事对于我们的用户和内部运维来说是完全透明的，系统自动修复不需要人的干预。
- 另一方面，如果修复不了，系统能够做自我保护，而不让事态变糟糕。

## 弹力设计篇之“隔离设计”
[42 弹力设计：隔离设计 - 极客时间文档](https://uaxe.github.io/geektime-docs/%E5%90%8E%E7%AB%AF-%E6%9E%B6%E6%9E%84/%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/42%20-%20%E5%BC%B9%E5%8A%9B%E8%AE%BE%E8%AE%A1%EF%BC%9A%E9%9A%94%E7%A6%BB%E8%AE%BE%E8%AE%A1/)
隔离设计对应的单词是 Bulkheads，中文翻译为隔板。
一般来说，对于系统的分离有两种方式，一种是以服务的种类来做分离，一种是以用户来做分离。

### 按服务的种类做分离
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/08d6d457bd5ec035bb613fa24f667d71_MD5.jpeg)
将系统分成了用户、商品、社区三个板块。这三个块分别使用不同的域名、服务器和数据库，做到从接入层到应用层再到数据层三层完全隔离。这样一来，在物理上来说，一个板块的故障就不会影响到另一板块。
Amazon中每个服务都有自己的数据库，每个数据库中都保存着和这个业务相关的数据和相应的处理状态。而每个服务从一开始就准备好了对外暴露。同时，这也是微服务所推荐的架构方式。
存在的问题是：
- 如果我们需要同时获得多个板块的数据，那么就需要调用多个服务，这会降低性能。注意，这里性能降低指的是响应时间，而不是吞吐量（相反，在这种架构下，吞吐量可以得到提高）。
这种情况下，可以通过设计用户交互来解决。
- 如果有大数据平台，就需要把这些数据都抽取到一个数据仓库中进行计算，这也增加了数据合并的复杂度。对于这个问题，我们需要一个框架或是一个中间件来对数据进行相应的抽取。
- 另外，如果我们的业务逻辑或是业务流程需要跨板块的话，那么一个板块的故障也会导致整个流程走不下去，同样会导致整体业务故障。
- 对于这个问题，一方面，我们需要保证这个业务流程中各个子系统的高可用性，并且在业务流程上做成 Step-by-Step 的方式，这样用户交互的每一步都可以保存，以便故障恢复后可以继续执行，而不是从头执行。
- 还有，如果需要有跨板块的交互也会变得有点复杂。对此我们需要一个类似于 Pub/Sub 的高可用、且可以持久化的消息订阅通知中间件来打通各个板块的数据和信息交换。
- 最后还会有在多个板块中分布式事务的问题。对此，我们需要“二阶段提交”这样的方案。在亚马逊中，使用的是 Plan – Reserve – Commit/Cancel 模式。
隔离了的系统需要引入大量的异步处理模型。

### 按用户的请求做分离
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/6918fa2e1e7e6fc70e5e49c1864844d8_MD5.jpeg)
将用户分成不同的组，并把后端的同一个服务根据这些不同的组分成不同的实例。让同一个服务对于不同的用户进行冗余和隔离，这样一来，当服务实例挂掉时，只会影响其中一部分用户，而不会导致所有的用户无法访问。
这种分离可以和按功能的分离进行融合。就是所谓的“多租户”模式。对于一些比较大的客户，我们可以为他们设置专门独立的服务实例，或是服务集群与其他客户隔离开来，对于一些比较小的用户来说，可以让他们共享一个服务实例，这样可以节省相关的资源。
通常来说多租户的做法有三种。
- 完全独立的设计。每个租户有自己完全独立的服务和数据。
- 独立的数据分区，共享的服务。多租户的服务是共享的，但数据是分开隔离的。
- 共享的服务，共享的数据分区。每个租户的数据和服务都是共享的。
![](%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E/attachments/12b308acb5aff2f13a54459222b87e95_MD5.jpeg)
- 如果使用完全独立的方案，在开发实现上和资源隔离度方面会非常好，然而，成本会比较高，计算资源也会有一定的浪费。
- 如果使用完全共享的方案，在资源利用和成本上会非常好，然而，开发难度非常大，而且数据和资源隔离非常不好。
一般来说，技术方案会使用折中方案，也就是中间方案，服务是共享的，数据通过分区来隔离，而对于一些比较重要的租户（需要好的隔离性），则使用完全独立的方式。
在虚拟化技术成熟的今天，我们完全可以使用“完全独立”（完全隔离）的方案，通过底层的虚拟化技术（Hypervisor 的技术，如 KVM，或是 Linux Container 的技术，如 Docker）来实现物理资源的共享和成本的节约。

### 隔离设计的重点
1. 定义隔离业务的大小和粒度
2. 无论是系统板块还是多租户的隔离，需要考虑系统的复杂度、成本、性能以及资源使用情况。
3. 隔离模式需要配置一些高可用、重试、异步、消息中间件，流控、熔断等设计模式的方式配套使用。
4. 需要自动化运维工具
5. 服务监控系统

## 弹力设计篇之“异步通讯设计”
