参考： 
1. https://w0279g7ggv.feishu.cn/file/Js8Rb84SeoWjtpxLoPrc2yE0nhb
2. [深入解析常见三次握手异常](https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247485995&idx=1&sn=aaa85a282d5a373efbb9e649465a9306&scene=21#wechat_redirect)

**目录**

- [1. 客户端connect函数中的异常](#1.%20%E5%AE%A2%E6%88%B7%E7%AB%AFconnect%E5%87%BD%E6%95%B0%E4%B8%AD%E7%9A%84%E5%BC%82%E5%B8%B8)
- [2. 第一次握手丢包](#2.%20%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8F%A1%E6%89%8B%E4%B8%A2%E5%8C%85)
	- [2.1 半连接队列满](#2.1%20%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E6%BB%A1)
	- [2.2 全连接队列满](#2.2%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E6%BB%A1)
	- [2.3 客户端发起重试](#2.3%20%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E8%B5%B7%E9%87%8D%E8%AF%95)
- [3. 第三次握手丢包](#3.%20%E7%AC%AC%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E4%B8%A2%E5%8C%85)
- [4. 总结以及优化](#4.%20%E6%80%BB%E7%BB%93%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96)


在后端接口性能指标中一类重要的指标就是接口耗时。具体包括平均响应时间 TP90、TP99 耗时值等。这些值越低越好，一般来说是几毫秒，或者是几十毫秒。如果响应时间一旦过长，比如超过了 1 秒，在用户侧就能感觉到非常明显的卡顿。
在正常情况下一次 TCP 连接耗时也就大约是一次 RTT 多一点。
## 1. 客户端connect函数中的异常
![](%E5%BC%80%E5%8F%91%E5%86%85%E5%8A%9F/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/attachments/24b06b25fb1b5888a2e9423fbeba21e8_MD5.jpeg)
端口号和 CPU 消耗这二者听起来感觉没啥太大联系。但是上图展示的情况就是因为端口号不足导致CPU消耗大幅上涨的情况。
客户端在发起 connect 系统调用的时候，主要工作就是端口选择。
在选择的过程中，有个大循环，从`ip_local_port_range`的一个随机位置开始把这个范围遍历一遍，找到可用端口则退出循环。如果端口很充足，那么循环只需要执行少数几次就可以退出。但假设说端口消耗掉很多已经不充足，或者干脆就没有可用的了。那么这个循环就得执行很多遍。我们来看下详细的代码。
``` cpp
//file:net/ipv4/inet_hashtables.c  
int __inet_hash_connect(...)  
{  
 inet_get_local_port_range(&low, &high);  
 remaining = (high - low) + 1;  
  
 for (i = 1; i <= remaining; i++) {  
  // 其中 offset 是一个随机数  
  port = low + (i + offset) % remaining;  
  head = &hinfo->bhash[inet_bhashfn(net, port,  
     hinfo->bhash_size)];  
  
  //加锁  
  spin_lock(&head->lock);   
  
  //一大段的选择端口逻辑  
  //......  
  //选择成功就 goto ok  
  //不成功就 goto next_port  
  
  next_port:  
   //解锁  
   spin_unlock(&head->lock);   
 }  
}
```
在每次的循环内部需要等待锁，以及在哈希表中执行多次的搜索。注意这里的是自旋锁，是一种非阻塞的锁，如果资源被占用，进程并不会被挂起，而是会占用 CPU 去不断尝试获取锁。
但假设端口范围 ip_local_port_range 配置的是 10000 - 30000， 而且已经用尽了。那么每次当发起连接的时候都需要把循环执行两万遍才退出。这时会涉及大量的 HASH 查找以及自旋锁等待开销，系统态 CPU 将会出现大幅度的上涨。
正常时的`connect`系统调用耗时大约是22us。
![](%E5%BC%80%E5%8F%91%E5%86%85%E5%8A%9F/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/attachments/49f06936efeee190dd9a1115edbe9d79_MD5.jpeg)
在端口不足的情况下`connect`的开销是2581us。
![](%E5%BC%80%E5%8F%91%E5%86%85%E5%8A%9F/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/attachments/1dc2ba75268e5847de0c4e2c80ebf996_MD5.jpeg)
异常情况下的 connect 耗时是正常情况下的100多倍。
## 2. 第一次握手丢包
服务器在响应来自客户端的第一次握手请求的时候，会判断一下半连接队列和全连接队列是否溢出。如果发生溢出，可能会直接将握手包丢弃，而不会反馈给客户端。
### 2.1 半连接队列满
``` cpp
//file: net/ipv4/tcp_ipv4.c  
int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)  
{  
 //看看半连接队列是否满了  
 if (inet_csk_reqsk_queue_is_full(sk) && !isn) {  
  want_cookie = tcp_syn_flood_action(sk, skb, "TCP");  
  if (!want_cookie)  
   goto drop;  
 }  
  
 //看看全连接队列是否满了  
 ...  
drop:  
 NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);  
 return 0;   
}
```
`inet_csk_reqsk_queue_is_full`如果返回true，则表示半连接队列满了。`tcp_syn_flood_action`判断是否打开内核参数`tcp_syncookies`，如果未打开则返回false。
``` cpp
//file: net/ipv4/tcp_ipv4.c  
bool tcp_syn_flood_action(...)  
{  
 bool want_cookie = false;  
  
 if (sysctl_tcp_syncookies) {  
  want_cookie = true;  
 }   
 return want_cookie;  
}
```
也就是说，**如果半连接队列满了，而且 ipv4.tcp_syncookies 参数设置为 0，那么来自客户端的握手包将 goto drop，意思就是直接丢弃！**
SYN Flood 攻击就是通过消耗光服务器上的半连接队列来使得正常的用户连接请求无法被响应。不过在现在的 Linux 内核里只要打开 tcp_syncookies，半连接队列满了仍然也还可以保证正常握手的进行。
### 2.2 全连接队列满
我们注意到当半连接队列判断通过以后，紧接着还有全连接队列满的相关判断。如果这个条件成立，服务器对握手包的处理还是会 goto drop，丢弃了之。
``` cpp
//file: net/ipv4/tcp_ipv4.c  
int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)  
{  
 //看看半连接队列是否满了  
 ...  
  
 //看看全连接队列是否满了  
 if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {  
  NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);  
  goto drop;  
 }  
 ...  
drop:  
 NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);  
 return 0;   
}
```
`sk_acceptq_is_full` 来判断全连接队列是否满了，`inet_csk_reqsk_queue_young` 判断的是有没有 `young_ack`（未处理完的半连接请求）。
这段代码可以看到，**假如全连接队列满的情况下，且同时有 `young_ack` ，那么内核同样直接丢掉该 SYN 握手包**。
### 2.3 客户端发起重试
假设说服务器侧发生了全/半连接队列溢出而导致的丢包。那么从转换到客户端视角来看就是 SYN 包没有任何响应。
客户端在发出握手包的时候，开启了一个重传定时器。如果收不到预期的 synack 的话，超时重传的逻辑就会开始执行。不过重传计时器的时间单位都是以秒来计算的（新版本中是1s，2.6版本是3s），这意味着，如果有握手重传发生，即使第一次重传就能成功，那接口最快响应也是 1 s 以后的事情了。这对接口耗时影响非常的大。
![](%E5%BC%80%E5%8F%91%E5%86%85%E5%8A%9F/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/attachments/82b56d37c6cef44e431177f9e271670e_MD5.jpeg)
如果能正常接收到服务器响应的 `synack`，那么客户端的这个定时器会清除。
如果服务器端发生了丢包，那么定时器到时后会进行回调函数 `tcp_write_timer` 中进行重传。
`tcp_retransmit_timer` 是重传的主要函数。在这里完成重传，以及下一次定时器到期时间的设置。
接着在 `tcp_retransmit_timer` 重发了发送队列里的头元素。而且还设置了下一次超时的时间，为前一次的两倍（左移操作相当于乘2）。
假如我们服务器上在第一次握手的时候出现了半/全连接队列溢出导致的丢包，那么我们的接口响应时间将至少是 1 s 以上（在某些老版本的内核上，SYN 第一次的重试就需要等 3 秒），如果连续两三次握手都失败，那 7，8 秒就出去了。你想想这对用户是不是影响很大。
## 3. 第三次握手丢包
客户端在收到服务器的 synack 响应的时候，就认为连接建立成功了，然后会将自己的连接状态设置为 ESTABLISHED，发出第三次握手请求。但服务器在第三次握手的时候，还有可能会有意外发生。
**第三次握手时，如果服务器全连接队列满了，来自客户端的 ack 握手包又被直接丢弃了**。
不过有意思的是，第三次握手失败并不是客户端重试，而是由客户端来重发 synack。
服务器等到半连接定时器到时后，向客户端重新发起 synack ，客户端收到后再重新回复第三次握手 ack。如果这期间服务器端全连接队列一直都是满的，那么服务器重试 5 次（受内核参数 net.ipv4.tcp_synack_retries 控制）后就放弃了。
在这种情况下大家还要注意另外一个问题。在实践中，客户端往往是以为连接建立成功就会开始发送数据，其实这时候连接还没有真的建立起来。他发出去的数据，包括重试都将全部被服务器无视。直到连接真正建立成功后才行。
## 4. 总结以及优化
1) 如何请求频繁，将短连接更换为长连接
2) 尽量合并请求
3) 注意`ip_local_port_range`要够用
4) 客户端连接不同的服务端，避免服务端端口不够用
5) 打开`SYN cookie`避免半连接队列溢出
6) 服务端通过backlog（全连接队列最大长度）和net.core.somaconn（最大连接队列长度，限制了所有backlog值）控制合适的连接队列长度
全连接队列的长度是 `min(backlog, net.core.somaxconn)`，半连接队列长度有点小复杂，是 `min(backlog, somaxconn, tcp_max_syn_backlog) + 1` 再上取整到 2 的幂次，但最小不能小于16。
7) 服务端尽快accept让应用取走连接
应用程序应该尽快在握手成功之后通过 accept 把新连接取走。不要忙于处理其它业务逻辑而导致全连接队列塞满了。